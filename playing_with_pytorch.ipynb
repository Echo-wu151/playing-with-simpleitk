{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "playing_with_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doj9EzIVIMFv",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz32_aKqJmIh",
        "colab_type": "text"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUCH6Ys5GLKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9452e1bc-aee3-45fd-fd28-8dc9970b9a3a"
      },
      "source": [
        "!pip install Google-Colab-Transfer"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Google-Colab-Transfer in /usr/local/lib/python3.6/dist-packages (0.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWi4McPs_NuY",
        "colab_type": "code",
        "outputId": "f8fa0593-9c17-45ab-dc8a-47d4c1971433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install SimpleITK"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.6/dist-packages (1.2.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQg7Mz4CGmjw",
        "colab_type": "code",
        "outputId": "c493bc7c-07ea-44be-a344-f0178f8adeb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us1qHdnsJlEQ",
        "colab_type": "code",
        "outputId": "f4dfd1f9-64fd-4ecb-e321-29c1bdbb63fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "!pip install git+https://github.com/ncullen93/torchsample"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ncullen93/torchsample\n",
            "  Cloning https://github.com/ncullen93/torchsample to /tmp/pip-req-build-yekd39rv\n",
            "  Running command git clone -q https://github.com/ncullen93/torchsample /tmp/pip-req-build-yekd39rv\n",
            "Requirement already satisfied (use --upgrade to upgrade): torchsample==0.1.3 from git+https://github.com/ncullen93/torchsample in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: torchsample\n",
            "  Building wheel for torchsample (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchsample: filename=torchsample-0.1.3-cp36-none-any.whl size=43417 sha256=f044deebf059a4706b5bd0c2b84488602b01ffebfce9fcb533fedac2bc213ce1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-csqmxcxq/wheels/88/c7/72/14cd9a173eed1e29d0b17d866e7d9ee511d31a834aedd27489\n",
            "Successfully built torchsample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AoxeZYwF46I",
        "colab_type": "text"
      },
      "source": [
        "## Data nomenclature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oFoBRkhIVVX",
        "colab_type": "text"
      },
      "source": [
        "Patients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AJwRH-8FmaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_patient_indices():\n",
        "  patient_indices = [1,2,5,6,8,10,14,16,18,19,21,22,23,24,25,26,27,28,29,30]\n",
        "\n",
        "  return patient_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P65kljpNIRK4",
        "colab_type": "text"
      },
      "source": [
        "Folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiLgnz7zFY13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_folder():\n",
        "  data_folder = 'data/'\n",
        "\n",
        "  return data_folder\n",
        "\n",
        "def get_image_folder():\n",
        "  image_folder = get_data_folder() + 'imagesTr/'\n",
        "\n",
        "  return image_folder\n",
        "\n",
        "def get_ground_truth_folder():\n",
        "  ground_truth_folder = get_data_folder() + 'labelsTr/'\n",
        "\n",
        "  return ground_truth_folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfQXankUIS88",
        "colab_type": "text"
      },
      "source": [
        "Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbGlq7B8biXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_file_extension():\n",
        "  file_extension = '.nii.gz'\n",
        "\n",
        "  return file_extension\n",
        "\n",
        "def get_image_file_name(patient_no, modality_no=0):\n",
        "  image_modality_convention = '_{:04.0f}'.format(modality_no)\n",
        "  image_file_name = get_image_folder() + 'patientID' + str(patient_no) + image_modality_convention + get_file_extension()\n",
        "\n",
        "  return  image_file_name\n",
        "\n",
        "def get_ground_truth_file_name(patient_no):\n",
        "  ground_truth_file_name = get_ground_truth_folder() +'patientID' + str(patient_no) + get_file_extension()\n",
        "\n",
        "  return  ground_truth_file_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMvKoAVsJPx5",
        "colab_type": "text"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDCyrb_oW-NB",
        "colab_type": "code",
        "outputId": "64cb38ad-9759-4858-f823-c31fbdbc0013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import colab_transfer\n",
        "\n",
        "colab_transfer.mount_google_drive()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HbPo9F8kB31W",
        "outputId": "9b7bf748-30ff-4408-995c-55be42c45875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "source": [
        "import colab_transfer\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "for folder_name in [ get_image_folder(), get_ground_truth_folder() ]:\n",
        "  Path(folder_name).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for patient_no in get_patient_indices():\n",
        "  for file_name in [ get_image_file_name(patient_no), get_ground_truth_file_name(patient_no) ]:\n",
        "    \n",
        "    colab_transfer.copy_file(file_name, \n",
        "              source=colab_transfer.get_path_to_home_of_google_drive(),\n",
        "              destination=colab_transfer.get_path_to_home_of_local_machine())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying /content/drive/My Drive/data/imagesTr/patientID1_0000.nii.gz to /content/data/imagesTr/patientID1_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID1.nii.gz to /content/data/labelsTr/patientID1.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID2_0000.nii.gz to /content/data/imagesTr/patientID2_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID2.nii.gz to /content/data/labelsTr/patientID2.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID5_0000.nii.gz to /content/data/imagesTr/patientID5_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID5.nii.gz to /content/data/labelsTr/patientID5.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID6_0000.nii.gz to /content/data/imagesTr/patientID6_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID6.nii.gz to /content/data/labelsTr/patientID6.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID8_0000.nii.gz to /content/data/imagesTr/patientID8_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID8.nii.gz to /content/data/labelsTr/patientID8.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID10_0000.nii.gz to /content/data/imagesTr/patientID10_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID10.nii.gz to /content/data/labelsTr/patientID10.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID14_0000.nii.gz to /content/data/imagesTr/patientID14_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID14.nii.gz to /content/data/labelsTr/patientID14.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID16_0000.nii.gz to /content/data/imagesTr/patientID16_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID16.nii.gz to /content/data/labelsTr/patientID16.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID18_0000.nii.gz to /content/data/imagesTr/patientID18_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID18.nii.gz to /content/data/labelsTr/patientID18.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID19_0000.nii.gz to /content/data/imagesTr/patientID19_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID19.nii.gz to /content/data/labelsTr/patientID19.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID21_0000.nii.gz to /content/data/imagesTr/patientID21_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID21.nii.gz to /content/data/labelsTr/patientID21.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID22_0000.nii.gz to /content/data/imagesTr/patientID22_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID22.nii.gz to /content/data/labelsTr/patientID22.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID23_0000.nii.gz to /content/data/imagesTr/patientID23_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID23.nii.gz to /content/data/labelsTr/patientID23.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID24_0000.nii.gz to /content/data/imagesTr/patientID24_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID24.nii.gz to /content/data/labelsTr/patientID24.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID25_0000.nii.gz to /content/data/imagesTr/patientID25_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID25.nii.gz to /content/data/labelsTr/patientID25.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID26_0000.nii.gz to /content/data/imagesTr/patientID26_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID26.nii.gz to /content/data/labelsTr/patientID26.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID27_0000.nii.gz to /content/data/imagesTr/patientID27_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID27.nii.gz to /content/data/labelsTr/patientID27.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID28_0000.nii.gz to /content/data/imagesTr/patientID28_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID28.nii.gz to /content/data/labelsTr/patientID28.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID29_0000.nii.gz to /content/data/imagesTr/patientID29_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID29.nii.gz to /content/data/labelsTr/patientID29.nii.gz\n",
            "Copying /content/drive/My Drive/data/imagesTr/patientID30_0000.nii.gz to /content/data/imagesTr/patientID30_0000.nii.gz\n",
            "Copying /content/drive/My Drive/data/labelsTr/patientID30.nii.gz to /content/data/labelsTr/patientID30.nii.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HGBrW0YIm94",
        "colab_type": "text"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sGD_c2Iq5Da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_folder = '/content/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgbS9WFvrJQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patient_indices = get_patient_indices()\n",
        "patient_no = patient_indices[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdlLa9Rf-luy",
        "colab_type": "code",
        "outputId": "4870577d-6788-4d57-c729-2e827e9fbecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "\n",
        "image_name = root_folder + get_image_file_name(patient_no)\n",
        "\n",
        "image = sitk.ReadImage(image_name)\n",
        "print(image.GetSize())\n",
        "\n",
        "v = sitk.GetArrayViewFromImage(image)\n",
        "\n",
        "np.unique(v)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(512, 512, 96)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1200, -1199, -1198, ...,  1235,  1251,  1264], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_X6uel5LMmS",
        "colab_type": "code",
        "outputId": "4122b221-3293-4fba-9d2d-e36f6aabde01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "\n",
        "image_name = root_folder + get_ground_truth_file_name(patient_no)\n",
        "\n",
        "image = sitk.ReadImage(image_name)\n",
        "print(image.GetSize())\n",
        "\n",
        "v = sitk.GetArrayViewFromImage(image)\n",
        "\n",
        "np.unique(v)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(512, 512, 96)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0, 255], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4Q8PKbvSA3Q",
        "colab_type": "text"
      },
      "source": [
        "## Down-sample images\n",
        "\n",
        "Reference: https://github.com/jonasteuwen/SimpleITK-examples/blob/master/examples/resample_isotropically.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMlj4oQoghC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/SimpleITK/SlicerSimpleFilters/blob/master/SimpleFilters/SimpleFilters.py\n",
        "_SITK_INTERPOLATOR_DICT = {\n",
        "    'nearest': sitk.sitkNearestNeighbor,\n",
        "    'linear': sitk.sitkLinear,\n",
        "    'gaussian': sitk.sitkGaussian,\n",
        "    'label_gaussian': sitk.sitkLabelGaussian,\n",
        "    'bspline': sitk.sitkBSpline,\n",
        "    'hamming_sinc': sitk.sitkHammingWindowedSinc,\n",
        "    'cosine_windowed_sinc': sitk.sitkCosineWindowedSinc,\n",
        "    'welch_windowed_sinc': sitk.sitkWelchWindowedSinc,\n",
        "    'lanczos_windowed_sinc': sitk.sitkLanczosWindowedSinc\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UfoZyepdjG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import SimpleITK as sitk\n",
        "import os\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def resample_sitk_image(sitk_image, spacing=None, interpolator=None,\n",
        "                        fill_value=0):\n",
        "    \"\"\"Resamples an ITK image to a new grid. If no spacing is given,\n",
        "    the resampling is done isotropically to the smallest value in the current\n",
        "    spacing. This is usually the in-plane resolution. If not given, the\n",
        "    interpolation is derived from the input data type. Binary input\n",
        "    (e.g., masks) are resampled with nearest neighbors, otherwise linear\n",
        "    interpolation is chosen.\n",
        "    Parameters\n",
        "    ----------\n",
        "    sitk_image : SimpleITK image or str\n",
        "      Either a SimpleITK image or a path to a SimpleITK readable file.\n",
        "    spacing : tuple\n",
        "      Tuple of integers\n",
        "    interpolator : str\n",
        "      Either `nearest`, `linear` or None.\n",
        "    fill_value : int\n",
        "    Returns\n",
        "    -------\n",
        "    SimpleITK image.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(sitk_image, str):\n",
        "        sitk_image = sitk.ReadImage(sitk_image)\n",
        "    num_dim = sitk_image.GetDimension()\n",
        "\n",
        "    if not interpolator:\n",
        "        interpolator = 'linear'\n",
        "        pixelid = sitk_image.GetPixelIDValue()\n",
        "\n",
        "        if pixelid not in [1, 2, 4]:\n",
        "            raise NotImplementedError(\n",
        "                'Set `interpolator` manually, '\n",
        "                'can only infer for 8-bit unsigned or 16, 32-bit signed integers')\n",
        "        if pixelid == 1: #  8-bit unsigned int\n",
        "            interpolator = 'nearest'\n",
        "\n",
        "    orig_pixelid = sitk_image.GetPixelIDValue()\n",
        "    orig_origin = sitk_image.GetOrigin()\n",
        "    orig_direction = sitk_image.GetDirection()\n",
        "    orig_spacing = np.array(sitk_image.GetSpacing())\n",
        "    orig_size = np.array(sitk_image.GetSize(), dtype=np.int)\n",
        "\n",
        "    if not spacing:\n",
        "        min_spacing = orig_spacing.min()\n",
        "        new_spacing = [min_spacing]*num_dim\n",
        "    else:\n",
        "        new_spacing = [float(s) for s in spacing]\n",
        "\n",
        "    assert interpolator in _SITK_INTERPOLATOR_DICT.keys(),\\\n",
        "        '`interpolator` should be one of {}'.format(_SITK_INTERPOLATOR_DICT.keys())\n",
        "\n",
        "    sitk_interpolator = _SITK_INTERPOLATOR_DICT[interpolator]\n",
        "\n",
        "    new_size = orig_size*(orig_spacing/new_spacing)\n",
        "    new_size = np.ceil(new_size).astype(np.int) #  Image dimensions are in integers\n",
        "    new_size = [int(s) for s in new_size] #  SimpleITK expects lists, not ndarrays\n",
        "\n",
        "    resample_filter = sitk.ResampleImageFilter()\n",
        "\n",
        "    resampled_sitk_image = resample_filter.Execute(sitk_image,\n",
        "                                                   new_size,\n",
        "                                                   sitk.Transform(),\n",
        "                                                   sitk_interpolator,\n",
        "                                                   orig_origin,\n",
        "                                                   new_spacing,\n",
        "                                                   orig_direction,\n",
        "                                                   fill_value,\n",
        "                                                   orig_pixelid)\n",
        "\n",
        "    return resampled_sitk_image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfa4he1uic2L",
        "colab_type": "text"
      },
      "source": [
        "Down-sample for faster checks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aru4odNwh8_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_dim = 3\n",
        "new_spacing = [4]*num_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UkmVby_hZTF",
        "colab_type": "code",
        "outputId": "4e64073d-2262-4204-f6c1-fadff8adeab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for patient_no in get_patient_indices():\n",
        "  \n",
        "  file_name = get_ground_truth_file_name(patient_no)     \n",
        "  \n",
        "  print(file_name)\n",
        "\n",
        "  original_image = sitk.ReadImage(file_name)\n",
        "\n",
        "  # Copy information w.r.t. original spacing\n",
        "  original_intensity_image = sitk.ReadImage(get_image_file_name(patient_no))\n",
        "  original_image.CopyInformation(original_intensity_image)\n",
        "\n",
        "  resampled_image = resample_sitk_image(original_image,\n",
        "                                        spacing=new_spacing)\n",
        "\n",
        "  print(original_image.GetSize())\n",
        "  print(resampled_image.GetSize())\n",
        "\n",
        "  sitk.WriteImage(resampled_image, file_name) "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/labelsTr/patientID1.nii.gz\n",
            "(512, 512, 96)\n",
            "(89, 89, 36)\n",
            "data/labelsTr/patientID2.nii.gz\n",
            "(512, 512, 81)\n",
            "(100, 100, 41)\n",
            "data/labelsTr/patientID5.nii.gz\n",
            "(512, 512, 95)\n",
            "(97, 97, 38)\n",
            "data/labelsTr/patientID6.nii.gz\n",
            "(512, 512, 88)\n",
            "(84, 84, 33)\n",
            "data/labelsTr/patientID8.nii.gz\n",
            "(512, 512, 123)\n",
            "(94, 94, 50)\n",
            "data/labelsTr/patientID10.nii.gz\n",
            "(512, 512, 110)\n",
            "(89, 89, 44)\n",
            "data/labelsTr/patientID14.nii.gz\n",
            "(512, 512, 95)\n",
            "(73, 73, 38)\n",
            "data/labelsTr/patientID16.nii.gz\n",
            "(512, 512, 91)\n",
            "(102, 102, 46)\n",
            "data/labelsTr/patientID18.nii.gz\n",
            "(512, 512, 111)\n",
            "(88, 88, 45)\n",
            "data/labelsTr/patientID19.nii.gz\n",
            "(512, 512, 90)\n",
            "(86, 86, 36)\n",
            "data/labelsTr/patientID21.nii.gz\n",
            "(512, 512, 257)\n",
            "(100, 100, 65)\n",
            "data/labelsTr/patientID22.nii.gz\n",
            "(512, 512, 101)\n",
            "(93, 93, 41)\n",
            "data/labelsTr/patientID23.nii.gz\n",
            "(512, 512, 240)\n",
            "(85, 85, 60)\n",
            "data/labelsTr/patientID24.nii.gz\n",
            "(512, 512, 123)\n",
            "(82, 82, 62)\n",
            "data/labelsTr/patientID25.nii.gz\n",
            "(512, 512, 235)\n",
            "(77, 77, 59)\n",
            "data/labelsTr/patientID26.nii.gz\n",
            "(512, 512, 114)\n",
            "(98, 98, 46)\n",
            "data/labelsTr/patientID27.nii.gz\n",
            "(512, 512, 242)\n",
            "(88, 88, 61)\n",
            "data/labelsTr/patientID28.nii.gz\n",
            "(512, 512, 102)\n",
            "(89, 89, 41)\n",
            "data/labelsTr/patientID29.nii.gz\n",
            "(512, 512, 214)\n",
            "(88, 88, 54)\n",
            "data/labelsTr/patientID30.nii.gz\n",
            "(512, 512, 266)\n",
            "(94, 94, 67)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh_I49fVTbpw",
        "colab_type": "code",
        "outputId": "60721bd7-a4e0-4d62-95d1-d342ba42263d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for patient_no in get_patient_indices():\n",
        "  \n",
        "  file_name = get_image_file_name(patient_no)\n",
        "\n",
        "  print(file_name)\n",
        "\n",
        "  original_image = sitk.ReadImage(file_name)\n",
        "  resampled_image = resample_sitk_image(original_image,\n",
        "                                        spacing=new_spacing)\n",
        "\n",
        "  print(original_image.GetSize())\n",
        "  print(resampled_image.GetSize())\n",
        "\n",
        "  sitk.WriteImage(resampled_image, file_name)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/imagesTr/patientID1_0000.nii.gz\n",
            "(512, 512, 96)\n",
            "(89, 89, 36)\n",
            "data/imagesTr/patientID2_0000.nii.gz\n",
            "(512, 512, 81)\n",
            "(100, 100, 41)\n",
            "data/imagesTr/patientID5_0000.nii.gz\n",
            "(512, 512, 95)\n",
            "(97, 97, 38)\n",
            "data/imagesTr/patientID6_0000.nii.gz\n",
            "(512, 512, 88)\n",
            "(84, 84, 33)\n",
            "data/imagesTr/patientID8_0000.nii.gz\n",
            "(512, 512, 123)\n",
            "(94, 94, 50)\n",
            "data/imagesTr/patientID10_0000.nii.gz\n",
            "(512, 512, 110)\n",
            "(89, 89, 44)\n",
            "data/imagesTr/patientID14_0000.nii.gz\n",
            "(512, 512, 95)\n",
            "(73, 73, 38)\n",
            "data/imagesTr/patientID16_0000.nii.gz\n",
            "(512, 512, 91)\n",
            "(102, 102, 46)\n",
            "data/imagesTr/patientID18_0000.nii.gz\n",
            "(512, 512, 111)\n",
            "(88, 88, 45)\n",
            "data/imagesTr/patientID19_0000.nii.gz\n",
            "(512, 512, 90)\n",
            "(86, 86, 36)\n",
            "data/imagesTr/patientID21_0000.nii.gz\n",
            "(512, 512, 257)\n",
            "(100, 100, 65)\n",
            "data/imagesTr/patientID22_0000.nii.gz\n",
            "(512, 512, 101)\n",
            "(93, 93, 41)\n",
            "data/imagesTr/patientID23_0000.nii.gz\n",
            "(512, 512, 240)\n",
            "(85, 85, 60)\n",
            "data/imagesTr/patientID24_0000.nii.gz\n",
            "(512, 512, 123)\n",
            "(82, 82, 62)\n",
            "data/imagesTr/patientID25_0000.nii.gz\n",
            "(512, 512, 235)\n",
            "(77, 77, 59)\n",
            "data/imagesTr/patientID26_0000.nii.gz\n",
            "(512, 512, 114)\n",
            "(98, 98, 46)\n",
            "data/imagesTr/patientID27_0000.nii.gz\n",
            "(512, 512, 242)\n",
            "(88, 88, 61)\n",
            "data/imagesTr/patientID28_0000.nii.gz\n",
            "(512, 512, 102)\n",
            "(89, 89, 41)\n",
            "data/imagesTr/patientID29_0000.nii.gz\n",
            "(512, 512, 214)\n",
            "(88, 88, 54)\n",
            "data/imagesTr/patientID30_0000.nii.gz\n",
            "(512, 512, 266)\n",
            "(94, 94, 67)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAyCWvIuu8TE",
        "colab_type": "text"
      },
      "source": [
        "[DONE] check label images after information was copied, to ensure orientation is correctly interpreted by ITK-SNAP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emU2Bj_aoOsc",
        "colab_type": "text"
      },
      "source": [
        "## Binarize label maps\n",
        "\n",
        "nnUNet wants consecutive labels: 0, 1, etc.\n",
        "\n",
        "Originally, the label map in the CHAOS challenge contains only two labels:\n",
        "- 0 (background),\n",
        "- 255 (region of interest).\n",
        "\n",
        "We can simply binarize the label map.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnOCG_CgoN-h",
        "colab_type": "code",
        "outputId": "444e0850-06a8-45e3-e237-3736fb1b084e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import SimpleITK as sitk\n",
        "import numpy as np\n",
        "\n",
        "for patient_no in get_patient_indices():\n",
        "  print('Patient n°{}'.format(patient_no))\n",
        "\n",
        "  input_image_name = get_ground_truth_file_name(patient_no)\n",
        "  input_image = sitk.ReadImage(input_image_name)\n",
        "  print('Image size: {}'.format(input_image.GetSize()))\n",
        "\n",
        "  v = sitk.GetArrayFromImage(input_image)\n",
        "  \n",
        "  labels = np.unique(v)\n",
        "  print('Labels: {}'.format(labels))\n",
        "\n",
        "  max_val = max(labels)\n",
        "  median_val = max_val/2\n",
        "\n",
        "  print(median_val)\n",
        "\n",
        "  # Binarize\n",
        "\n",
        "  binarized_v = np.zeros(v.shape, v.dtype)\n",
        "  binarized_v[v>median_val] = 1\n",
        "\n",
        "  labels = np.unique(binarized_v)\n",
        "  print('Labels: {}'.format(labels))  \n",
        "\n",
        "  output_image = sitk.GetImageFromArray(binarized_v)\n",
        "  \n",
        "  # Copy meta-data\n",
        "  output_image.CopyInformation(input_image)\n",
        "\n",
        "  output_image_name = input_image_name\n",
        "  sitk.WriteImage(output_image, output_image_name)\n",
        "  print('Image size: {}'.format(output_image.GetSize()))\n",
        "  "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Patient n°1\n",
            "Image size: (89, 89, 36)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (89, 89, 36)\n",
            "Patient n°2\n",
            "Image size: (100, 100, 41)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (100, 100, 41)\n",
            "Patient n°5\n",
            "Image size: (97, 97, 38)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (97, 97, 38)\n",
            "Patient n°6\n",
            "Image size: (84, 84, 33)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (84, 84, 33)\n",
            "Patient n°8\n",
            "Image size: (94, 94, 50)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (94, 94, 50)\n",
            "Patient n°10\n",
            "Image size: (89, 89, 44)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (89, 89, 44)\n",
            "Patient n°14\n",
            "Image size: (73, 73, 38)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (73, 73, 38)\n",
            "Patient n°16\n",
            "Image size: (102, 102, 46)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (102, 102, 46)\n",
            "Patient n°18\n",
            "Image size: (88, 88, 45)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (88, 88, 45)\n",
            "Patient n°19\n",
            "Image size: (86, 86, 36)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (86, 86, 36)\n",
            "Patient n°21\n",
            "Image size: (100, 100, 65)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (100, 100, 65)\n",
            "Patient n°22\n",
            "Image size: (93, 93, 41)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (93, 93, 41)\n",
            "Patient n°23\n",
            "Image size: (85, 85, 60)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (85, 85, 60)\n",
            "Patient n°24\n",
            "Image size: (82, 82, 62)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (82, 82, 62)\n",
            "Patient n°25\n",
            "Image size: (77, 77, 59)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (77, 77, 59)\n",
            "Patient n°26\n",
            "Image size: (98, 98, 46)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (98, 98, 46)\n",
            "Patient n°27\n",
            "Image size: (88, 88, 61)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (88, 88, 61)\n",
            "Patient n°28\n",
            "Image size: (89, 89, 41)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (89, 89, 41)\n",
            "Patient n°29\n",
            "Image size: (88, 88, 54)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (88, 88, 54)\n",
            "Patient n°30\n",
            "Image size: (94, 94, 67)\n",
            "Labels: [  0 255]\n",
            "127.5\n",
            "Labels: [0 1]\n",
            "Image size: (94, 94, 67)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9SWg7ClGxXs",
        "colab_type": "text"
      },
      "source": [
        "## nnUNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-2_JJlaLqZD",
        "colab_type": "text"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-GSoWfWG0WE",
        "colab_type": "code",
        "outputId": "bc9c3671-b905-43b9-c98e-13ccc38b8a92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!git clone https://github.com/MIC-DKFZ/nnUNet.git\n",
        "!git clone https://github.com/woctezuma/nnUNet.git\n",
        "%cd nnUNet/\n",
        "!git checkout google-colab\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e .\n",
        "%cd nnunet/"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nnUNet'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 785 (delta 20), reused 28 (delta 12), pack-reused 744\u001b[K\n",
            "Receiving objects: 100% (785/785), 232.37 KiB | 94.00 KiB/s, done.\n",
            "Resolving deltas: 100% (548/548), done.\n",
            "/content/nnUNet\n",
            "Branch 'google-colab' set up to track remote branch 'google-colab' from 'origin'.\n",
            "Switched to a new branch 'google-colab'\n",
            "Requirement already satisfied: torch>1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.28.1)\n",
            "Collecting dicom2nifti\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/e3/71890b255abad2ce785b7af9935f80726f6dbf020d36e142e1d18c57b266/dicom2nifti-2.1.9.tar.gz\n",
            "Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.15.0)\n",
            "Collecting medpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/70/c1fd5dd60242eee81774696ea7ba4caafac2bad8f028bba94b1af83777d7/MedPy-0.4.0.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.3.3)\n",
            "Collecting batchgenerators>=0.19.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/3a/b8b7086e2594cf127464f6313f161b84ccd4c8fdfa1c37c2411bebda5cae/batchgenerators-0.19.5.tar.gz (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.17.4)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.0)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (1.2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (0.25.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from dicom2nifti->-r requirements.txt (line 3)) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from dicom2nifti->-r requirements.txt (line 3)) (0.16.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.6/dist-packages (from dicom2nifti->-r requirements.txt (line 3)) (2.3.3)\n",
            "Collecting pydicom>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/d5/da1fdf3b967e324ee47a7ad9553c9b94c1193b6b98afd9eeda0efb76b9f7/pydicom-1.3.0-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 22.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->-r requirements.txt (line 4)) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->-r requirements.txt (line 4)) (3.1.2)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->-r requirements.txt (line 4)) (4.3.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->-r requirements.txt (line 4)) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->-r requirements.txt (line 4)) (2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from batchgenerators>=0.19.4->-r requirements.txt (line 7)) (0.21.3)\n",
            "Collecting unittest2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/20/7f0f433060a962200b7272b8c12ba90ef5b903e218174301d0abfd523813/unittest2-1.1.0-py2.py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.7MB/s \n",
            "\u001b[?25hCollecting threadpoolctl\n",
            "  Downloading https://files.pythonhosted.org/packages/db/09/cab2f398e28e9f183714afde872b2ce23629f5833e467b151f18e1e08908/threadpoolctl-2.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 11)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 11)) (2.6.1)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from nibabel->dicom2nifti->-r requirements.txt (line 3)) (0.98)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->-r requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->-r requirements.txt (line 4)) (2.4.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit-image>=0.14->-r requirements.txt (line 4)) (0.46)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.14->-r requirements.txt (line 4)) (4.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->batchgenerators>=0.19.4->-r requirements.txt (line 7)) (0.14.1)\n",
            "Collecting argparse\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
            "Collecting traceback2\n",
            "  Downloading https://files.pythonhosted.org/packages/17/0a/6ac05a3723017a967193456a2efa0aa9ac4b51456891af1e2353bb9de21e/traceback2-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->-r requirements.txt (line 4)) (42.0.2)\n",
            "Collecting linecache2\n",
            "  Downloading https://files.pythonhosted.org/packages/c7/a3/c5da2a44c85bfbb6eebcfc1dde24933f8704441b98fdde6528f4831757a6/linecache2-1.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: dicom2nifti, medpy, batchgenerators\n",
            "  Building wheel for dicom2nifti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dicom2nifti: filename=dicom2nifti-2.1.9-cp36-none-any.whl size=41384 sha256=ef49269161fd143e8ea9dd5e094ae5641a7e60b25ee0ab8f9b3b9291ad11093f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/02/2f/e94223dcbdb566149e738087e869d9779fd3ce91ac61aa3baa\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.4.0-cp36-cp36m-linux_x86_64.whl size=753431 sha256=e6b8de4b6cef60ff9dd4b5389dd0bff9ac6f77d8ec5a35d7cfa099210e8b033f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/c9/9c/2c6281c7a72b9fb1570862a4f028af7ce38405008354fbf870\n",
            "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for batchgenerators: filename=batchgenerators-0.19.5-cp36-none-any.whl size=89726 sha256=d5073d7ff87a01f81158868bd5fb307a95909699226a5c6663b1b65fa56aefd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/15/62/b83aacbde15a7c5cfd2849cabe84f43746f8fc128992896d94\n",
            "Successfully built dicom2nifti medpy batchgenerators\n",
            "Installing collected packages: pydicom, dicom2nifti, medpy, argparse, linecache2, traceback2, unittest2, threadpoolctl, batchgenerators\n",
            "Successfully installed argparse-1.4.0 batchgenerators-0.19.5 dicom2nifti-2.1.9 linecache2-1.0.0 medpy-0.4.0 pydicom-1.3.0 threadpoolctl-2.0.0 traceback2-1.4.0 unittest2-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/nnUNet\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from nnunet==0.6) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nnunet==0.6) (4.28.1)\n",
            "Requirement already satisfied: dicom2nifti in /usr/local/lib/python3.6/dist-packages (from nnunet==0.6) (2.1.9)\n",
            "Requirement already satisfied: scikit-image>=0.14 in /usr/local/lib/python3.6/dist-packages (from nnunet==0.6) (0.15.0)\n",
            "Requirement already satisfied: medpy in /usr/local/lib/python3.6/dist-packages (from nnunet==0.6) (0.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from nnunet==0.6) (1.3.3)\n",
            "Requirement already satisfied: batchgenerators>=0.19.4 in /usr/local/lib/python3.6/dist-packages (from nnunet==0.6) (0.19.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nnunet==0.6) (1.17.4)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from nnunet==0.6) (0.0)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.6/dist-packages (from nnunet==0.6) (1.2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nnunet==0.6) (0.25.3)\n",
            "Requirement already satisfied: pydicom>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from dicom2nifti->nnunet==0.6) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from dicom2nifti->nnunet==0.6) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from dicom2nifti->nnunet==0.6) (1.12.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.6/dist-packages (from dicom2nifti->nnunet==0.6) (2.3.3)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->nnunet==0.6) (4.3.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->nnunet==0.6) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->nnunet==0.6) (3.1.2)\n",
            "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->nnunet==0.6) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14->nnunet==0.6) (2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from batchgenerators>=0.19.4->nnunet==0.6) (0.21.3)\n",
            "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.6/dist-packages (from batchgenerators>=0.19.4->nnunet==0.6) (2.0.0)\n",
            "Requirement already satisfied: unittest2 in /usr/local/lib/python3.6/dist-packages (from batchgenerators>=0.19.4->nnunet==0.6) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nnunet==0.6) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->nnunet==0.6) (2.6.1)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from nibabel->dicom2nifti->nnunet==0.6) (0.98)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit-image>=0.14->nnunet==0.6) (0.46)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet==0.6) (2.4.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet==0.6) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet==0.6) (0.10.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.14->nnunet==0.6) (4.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->batchgenerators>=0.19.4->nnunet==0.6) (0.14.1)\n",
            "Requirement already satisfied: argparse in /usr/local/lib/python3.6/dist-packages (from unittest2->batchgenerators>=0.19.4->nnunet==0.6) (1.4.0)\n",
            "Requirement already satisfied: traceback2 in /usr/local/lib/python3.6/dist-packages (from unittest2->batchgenerators>=0.19.4->nnunet==0.6) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.14->nnunet==0.6) (42.0.2)\n",
            "Requirement already satisfied: linecache2 in /usr/local/lib/python3.6/dist-packages (from traceback2->unittest2->batchgenerators>=0.19.4->nnunet==0.6) (1.0.0)\n",
            "Installing collected packages: nnunet\n",
            "  Running setup.py develop for nnunet\n",
            "Successfully installed nnunet\n",
            "/content/nnUNet/nnunet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB84bhDlLsor",
        "colab_type": "text"
      },
      "source": [
        "### Edit paths.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pZmKYNiH1ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Change lines with os.environ() in paths.py\n",
        "base = \"/content/nnUNet_base/\"\n",
        "preprocessing_output_dir = \"/content/nnUNet_preprocessed/\"\n",
        "network_training_output_dir = \"/content/RESULTS_FOLDER/\"\n",
        "\n",
        "# NB : if you use the google-colab branch in the forked repository, then you do\n",
        "#      not have to edit paths.py by hand."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AScgztwpS4eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "Path(preprocessing_output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "# NB : if you use the google-colab branch in the forked repository, then you do\n",
        "#      not have to create this directory by hand."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2NFuqonH5Uo",
        "colab_type": "code",
        "outputId": "d788d318-9012-4643-dff2-fb81ddd51e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from batchgenerators.utilities.file_and_folder_operations import join\n",
        "\n",
        "raw_dataset_dir = join(base, \"nnUNet_raw\")\n",
        "splitted_4d_output_dir = join(base, \"nnUNet_raw_splitted\")\n",
        "\n",
        "print(raw_dataset_dir)\n",
        "print(splitted_4d_output_dir)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/nnUNet_base/nnUNet_raw\n",
            "/content/nnUNet_base/nnUNet_raw_splitted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsZ2Ug5WLN8b",
        "colab_type": "text"
      },
      "source": [
        "### Create repositories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsKJSCYIK7o2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python paths.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOP7FXOdLP4G",
        "colab_type": "text"
      },
      "source": [
        "### Copy data to the right folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EHGKR_uKTNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/data /content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jdfq0ZASGDX",
        "colab_type": "text"
      },
      "source": [
        "## Edit dataset.json at the root of Task00_MY_DATASET/\n",
        "\n",
        "Reference: https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/dataset_conversion/LiverTumorSegmentationChallenge.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrd4WM8CsFLB",
        "colab_type": "code",
        "outputId": "b85fd82a-f560-49ea-f323-3cdd1befb9b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "output_folder = \"/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/\"\n",
        "\n",
        "train_ids = [\n",
        "             'patientID{}'.format(patient_no) \n",
        "             for patient_no in get_patient_indices()\n",
        "]\n",
        "\n",
        "print(train_ids)\n",
        "\n",
        "test_ids = [\n",
        "            \n",
        "]\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['patientID1', 'patientID2', 'patientID5', 'patientID6', 'patientID8', 'patientID10', 'patientID14', 'patientID16', 'patientID18', 'patientID19', 'patientID21', 'patientID22', 'patientID23', 'patientID24', 'patientID25', 'patientID26', 'patientID27', 'patientID28', 'patientID29', 'patientID30']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucr2QyYKailz",
        "colab_type": "text"
      },
      "source": [
        "Decrease the number of patients for faster check of the program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRnuGfVNahtS",
        "colab_type": "code",
        "outputId": "2d04da02-d2ad-4060-e939-72cd08e1ee0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "faster_check = True\n",
        "num_patients = 5\n",
        "\n",
        "if faster_check:\n",
        "  train_ids = train_ids[:5]\n",
        "\n",
        "  print(train_ids)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['patientID1', 'patientID2', 'patientID5', 'patientID6', 'patientID8']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY3xmwWBTcKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "\n",
        "json_dict = OrderedDict()\n",
        "json_dict['name'] = \"CHAOS\"\n",
        "json_dict['description'] = \"Combined (CT-MR) Healthy Abdominal Organ Segmentation\"\n",
        "json_dict['tensorImageSize'] = \"3D\"\n",
        "json_dict['reference'] = \"see challenge website\"\n",
        "json_dict['licence'] = \"see challenge website\"\n",
        "json_dict['release'] = \"0.0\"\n",
        "json_dict['modality'] = {\n",
        "    \"0\": \"CT\"\n",
        "}\n",
        "\n",
        "json_dict['labels'] = {\n",
        "    \"0\": \"background\",\n",
        "    \"1\": \"liver\",\n",
        "}\n",
        "\n",
        "json_dict['numTraining'] = len(train_ids)\n",
        "json_dict['numTest'] = len(test_ids)\n",
        "json_dict['training'] = [{'image': \"./imagesTr/%s.nii.gz\" % i, \"label\": \"./labelsTr/%s.nii.gz\" % i} for i in train_ids]\n",
        "json_dict['test'] = [\"./imagesTs/%s.nii.gz\" % i for i in test_ids]\n",
        "\n",
        "with open(os.path.join(output_folder, \"dataset.json\"), 'w') as f:\n",
        "    json.dump(json_dict, f, indent=4, sort_keys=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhKCU_jl0gIU",
        "colab_type": "text"
      },
      "source": [
        "## How many processes (pl, pf) on Google Colab?\n",
        "\n",
        "Apparently, 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj-lOLy26lJt",
        "colab_type": "code",
        "outputId": "79a6a0f7-9552-4b5f-cf29-da90c9ed72c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMPLpB3Utsum",
        "colab_type": "text"
      },
      "source": [
        "## Run pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns3QCWdfu8Ja",
        "colab_type": "code",
        "outputId": "7a0aa785-497d-4173-f789-6eeb10508280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        }
      },
      "source": [
        "!python experiment_planning/plan_and_preprocess_task.py -h"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, Fabian, et al. \"nnU-Net: Breaking the Spell on Successful Medical Image Segmentation.\" arXiv preprint arXiv:1904.08128 (2019).\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "usage: plan_and_preprocess_task.py [-h] -t TASK [-pl PROCESSES_LOWRES]\n",
            "                                   [-pf PROCESSES_FULLRES] [-o OVERRIDE]\n",
            "                                   [-s USE_SPLITTED]\n",
            "                                   [-no_preprocessing NO_PREPROCESSING]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -t TASK, --task TASK  task name. There must be a matching folder in\n",
            "                        raw_dataset_dir\n",
            "  -pl PROCESSES_LOWRES, --processes_lowres PROCESSES_LOWRES\n",
            "                        number of processes used for preprocessing 3d_lowres\n",
            "                        data, image splitting and image cropping Default: 8.\n",
            "                        The distinction between processes_lowres and\n",
            "                        processes_fullres is necessary because preprocessing\n",
            "                        at full resolution needs a lot of RAM\n",
            "  -pf PROCESSES_FULLRES, --processes_fullres PROCESSES_FULLRES\n",
            "                        number of processes used for preprocessing 2d and\n",
            "                        3d_fullres data. Default: 3\n",
            "  -o OVERRIDE, --override OVERRIDE\n",
            "                        set this to 1 if you want to override cropped data and\n",
            "                        intensityproperties. Default: 0\n",
            "  -s USE_SPLITTED, --use_splitted USE_SPLITTED\n",
            "                        1 = use splitted data if already present (skip\n",
            "                        split_4d). 0 = do splitting again. It is save to set\n",
            "                        this to 1 at all times unless the dataset was updated\n",
            "                        in the meantime. Default: 1\n",
            "  -no_preprocessing NO_PREPROCESSING\n",
            "                        debug only. If set to 1 this will run onlyexperiment\n",
            "                        planning and not run the preprocessing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGoE8e6Cttnf",
        "colab_type": "code",
        "outputId": "63b82154-f37c-441c-cd33-5db49cec8809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "run_pre_processing = False\n",
        "\n",
        "if faster_check:\n",
        "  # Force running the pre-processing\n",
        "  run_pre_processing = True\n",
        "\n",
        "if run_pre_processing:\n",
        "  ## Default: -pl 8 -pf 3\n",
        "  !OMP_NUM_THREADS=1 python experiment_planning/plan_and_preprocess_task.py -t Task00_MY_DATASET -pl 2 -pf 2"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, Fabian, et al. \"nnU-Net: Breaking the Spell on Successful Medical Image Segmentation.\" arXiv preprint arXiv:1904.08128 (2019).\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "patientID1\n",
            "patientID2\n",
            "before crop: (1, 36, 89, 89) after crop: (1, 36, 88, 88) spacing: [4. 4. 4.] \n",
            "\n",
            "before crop: (1, 41, 100, 100) after crop: (1, 41, 100, 100) spacing: [4. 4. 4.] \n",
            "\n",
            "patientID5\n",
            "patientID6\n",
            "before crop: (1, 38, 97, 97) after crop: (1, 38, 97, 97) spacing: [4. 4. 4.] \n",
            "\n",
            "before crop: (1, 33, 84, 84) after crop: (1, 33, 84, 84) spacing: [4. 4. 4.] \n",
            "\n",
            "patientID8\n",
            "before crop: (1, 50, 94, 94) after crop: (1, 49, 94, 94) spacing: [4. 4. 4.] \n",
            "\n",
            "Are we using the nonzero maks for normalizaion? OrderedDict([(0, False)])\n",
            "the median shape of the dataset is  [38. 94. 94.]\n",
            "the max shape in the dataset is  [ 49. 100. 100.]\n",
            "the min shape in the dataset is  [33. 84. 84.]\n",
            "we don't want feature maps smaller than  4  in the bottleneck\n",
            "the transposed median shape of the dataset is  [38. 94. 94.]\n",
            "{0: {'batch_size': 2, 'num_pool_per_axis': [3, 4, 4], 'patch_size': array([40, 96, 96]), 'median_patient_size_in_voxels': array([38, 94, 94]), 'current_spacing': array([4., 4., 4.]), 'original_spacing': array([4., 4., 4.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}}\n",
            "determining postprocessing...\n",
            "Postprocessing: only_keep_largest_connected_component OrderedDict([((1,), True)])\n",
            "Postprocessing: min_size_per_class OrderedDict([(1, 699477.76)])\n",
            "Postprocessing: min_region_size_per_class OrderedDict([(1, 699477.76)])\n",
            "Initializing to run preprocessing\n",
            "npz folder: /content/nnUNet_base/nnUNet_raw_cropped/Task00_MY_DATASET\n",
            "output_folder: /content/nnUNet_preprocessed/Task00_MY_DATASET\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 36, 88, 88)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 36, 88, 88)} \n",
            "\n",
            "normalization...\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "normalization done\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 41, 100, 100)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 41, 100, 100)} \n",
            "\n",
            "normalization...\n",
            "saving:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_stage0/patientID1.npz\n",
            "normalization done\n",
            "saving:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_stage0/patientID2.npz\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 38, 97, 97)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 38, 97, 97)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "saving:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_stage0/patientID5.npz\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 33, 84, 84)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 33, 84, 84)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "saving:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_stage0/patientID6.npz\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 49, 94, 94)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 49, 94, 94)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "saving:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_stage0/patientID8.npz\n",
            "Are we using the nonzero maks for normalizaion? OrderedDict([(0, False)])\n",
            "the median shape of the dataset is  [38. 94. 94.]\n",
            "the max shape in the dataset is  [ 49. 100. 100.]\n",
            "the min shape in the dataset is  [33. 84. 84.]\n",
            "we don't want feature maps smaller than  4  in the bottleneck\n",
            "the transposed median shape of the dataset is  [38. 94. 94.]\n",
            "[{'batch_size': 9, 'num_pool_per_axis': [4, 4], 'patch_size': array([96, 96]), 'median_patient_size_in_voxels': array([38, 94, 94]), 'current_spacing': array([4., 4., 4.]), 'original_spacing': array([4., 4., 4.]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}]\n",
            "determining postprocessing...\n",
            "Postprocessing: only_keep_largest_connected_component OrderedDict([((1,), True)])\n",
            "Postprocessing: min_size_per_class OrderedDict([(1, 699477.76)])\n",
            "Postprocessing: min_region_size_per_class OrderedDict([(1, 699477.76)])\n",
            "Initializing to run preprocessing\n",
            "npz folder: /content/nnUNet_base/nnUNet_raw_cropped/Task00_MY_DATASET\n",
            "output_folder: /content/nnUNet_preprocessed/Task00_MY_DATASET\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 36, 88, 88)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 36, 88, 88)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "saving:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_2D_stage0/patientID1.npz\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 41, 100, 100)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 41, 100, 100)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "saving:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_2D_stage0/patientID2.npz\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 38, 97, 97)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 38, 97, 97)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "saving:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_2D_stage0/patientID5.npz\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 33, 84, 84)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 33, 84, 84)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "saving:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_2D_stage0/patientID6.npz\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 49, 94, 94)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 49, 94, 94)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "saving:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_2D_stage0/patientID8.npz\n",
            "nnUNet_2D_stage0\n",
            "/content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_2D_stage0/patientID1.pkl\n",
            "/content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_2D_stage0/patientID2.pkl\n",
            "/content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_2D_stage0/patientID5.pkl\n",
            "/content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_2D_stage0/patientID6.pkl\n",
            "/content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_2D_stage0/patientID8.pkl\n",
            "nnUNet_stage0\n",
            "/content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_stage0/patientID1.pkl\n",
            "/content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_stage0/patientID2.pkl\n",
            "/content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_stage0/patientID5.pkl\n",
            "/content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_stage0/patientID6.pkl\n",
            "/content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_stage0/patientID8.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "styTP9bJWZLU",
        "colab_type": "text"
      },
      "source": [
        "Save to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFZwDGkfGC5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "local_data_path = '/content/nnUNet_preprocessed/Task00_MY_DATASET/'\n",
        "gdrive_data_path = '/content/drive/My Drive/nnUNet_preprocessed/Task00_MY_DATASET/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHGa15yTWojP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import colab_transfer\n",
        "\n",
        "if not faster_check:\n",
        "  colab_transfer.copy_folder_structure(source=local_data_path,\n",
        "                                       destination=gdrive_data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6lKqLzIWcpM",
        "colab_type": "text"
      },
      "source": [
        "Load from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS8xyrIXXbLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import colab_transfer\n",
        "\n",
        "if not faster_check:\n",
        "  colab_transfer.copy_folder_structure(source=gdrive_data_path,\n",
        "                                       destination=local_data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iujaOgAUFh0Z",
        "colab_type": "code",
        "outputId": "75ed4132-f2b9-4f0e-9839-6fae3e163e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!ls /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet_2D_stage0/"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "patientID1.npz\tpatientID2.npz\tpatientID5.npz\tpatientID6.npz\tpatientID8.npz\n",
            "patientID1.pkl\tpatientID2.pkl\tpatientID5.pkl\tpatientID6.pkl\tpatientID8.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg276gkO2CVS",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns26aJUi2O1V",
        "colab_type": "code",
        "outputId": "6f89be41-c0aa-4a21-f739-a40e7ad0e619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "!python run/run_training.py -h"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, Fabian, et al. \"nnU-Net: Breaking the Spell on Successful Medical Image Segmentation.\" arXiv preprint arXiv:1904.08128 (2019).\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "usage: run_training.py [-h] [-val] [-c] [-p P] [-u UNPACK_DATA] [--ndet]\n",
            "                       [--npz] [--find_lr] [--valbest] [--fp16]\n",
            "                       network network_trainer task fold\n",
            "\n",
            "positional arguments:\n",
            "  network\n",
            "  network_trainer\n",
            "  task\n",
            "  fold                  0, 1, ..., 5 or 'all'\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -val, --validation_only\n",
            "                        use this if you want to only run the validation\n",
            "  -c, --continue_training\n",
            "                        use this if you want to continue a training\n",
            "  -p P                  plans identifier\n",
            "  -u UNPACK_DATA, --unpack_data UNPACK_DATA\n",
            "                        Leave it as 1, development only\n",
            "  --ndet                Per default training is deterministic,\n",
            "                        nondeterministic allows cudnn.benchmark which will can\n",
            "                        give up to 20% performance. Set this to do\n",
            "                        nondeterministic training\n",
            "  --npz                 if set then nnUNet will export npz files of predicted\n",
            "                        segmentations in the vlaidation as well. This is\n",
            "                        needed to run the ensembling step so unless you are\n",
            "                        developing nnUNet you should enable this\n",
            "  --find_lr             not used here, just for fun\n",
            "  --valbest             hands off. This is not intended to be used\n",
            "  --fp16                enable fp16 training. Makes sense for 2d only! (and\n",
            "                        only on supported hardware!)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC_sIpdZ2E8I",
        "colab_type": "code",
        "outputId": "2ac0374c-d278-449f-dca8-cb9eaeb2b7e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!OMP_NUM_THREADS=1 python run/run_training.py 3d_fullres nnUNetTrainer Task00_MY_DATASET all --ndet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, Fabian, et al. \"nnU-Net: Breaking the Spell on Successful Medical Image Segmentation.\" arXiv preprint arXiv:1904.08128 (2019).\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "###############################################\n",
            "I am running the following nnUNet: 3d_fullres\n",
            "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer'>\n",
            "For that I will be using the following configuration:\n",
            "num_classes:  1\n",
            "modalities:  {0: 'CT'}\n",
            "use_mask_for_norm OrderedDict([(0, False)])\n",
            "keep_only_largest_region OrderedDict([((1,), True)])\n",
            "min_region_size_per_class OrderedDict([(1, 699477.76)])\n",
            "min_size_per_class OrderedDict([(1, 699477.76)])\n",
            "normalization_schemes OrderedDict([(0, 'CT')])\n",
            "stages...\n",
            "\n",
            "stage:  0\n",
            "{'batch_size': 2, 'num_pool_per_axis': [3, 4, 4], 'patch_size': array([40, 96, 96]), 'median_patient_size_in_voxels': array([38, 94, 94]), 'current_spacing': array([4., 4., 4.]), 'original_spacing': array([4., 4., 4.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
            "\n",
            "I am using stage 0 from these plans\n",
            "I am using sample dice + CE loss\n",
            "\n",
            "I am using data from this folder:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet\n",
            "###############################################\n",
            "2019-12-17 12:00:45.219918: Creating new split...\n",
            "2019-12-17 12:00:45.221327: unpacking dataset\n",
            "2019-12-17 12:00:45.375278: done\n",
            "2019-12-17 12:00:51.292570: Unable to plot network architecture:\n",
            "2019-12-17 12:00:51.292704: No module named 'hiddenlayer'\n",
            "2019-12-17 12:00:51.292923: \n",
            "epoch:  0\n",
            "gpu 0\n",
            "2019-12-17 12:10:21.404162: train loss : -0.5439\n",
            "gpu 0\n",
            "2019-12-17 12:10:53.949218: val loss (train=False): -0.7994\n",
            "2019-12-17 12:10:53.949947: Val glob dc per class: [0.9358583383234286]\n",
            "2019-12-17 12:10:55.166600: lr is now (scheduler) 0.0003\n",
            "2019-12-17 12:10:55.166845: current best_val_eval_criterion_MA is 0.93590\n",
            "2019-12-17 12:10:55.166970: current val_eval_criterion_MA is 0.9359\n",
            "2019-12-17 12:10:55.167129: No improvement: current train MA -0.5439, best: -0.5439, eps is 0.0005\n",
            "2019-12-17 12:10:55.167231: Patience: 0/50\n",
            "2019-12-17 12:10:55.167341: This epoch took 602.656597 s\n",
            "\n",
            "2019-12-17 12:10:55.167454: \n",
            "epoch:  1\n",
            "2019-12-17 12:20:11.349642: train loss : -0.8239\n",
            "2019-12-17 12:20:43.529442: val loss (train=False): -0.8885\n",
            "2019-12-17 12:20:43.529967: Val glob dc per class: [0.9537089894095695]\n",
            "2019-12-17 12:20:44.762265: lr is now (scheduler) 0.0003\n",
            "2019-12-17 12:20:44.762501: current best_val_eval_criterion_MA is 0.93590\n",
            "2019-12-17 12:20:44.762596: current val_eval_criterion_MA is 0.9376\n",
            "2019-12-17 12:20:44.762684: saving best epoch checkpoint...\n",
            "2019-12-17 12:20:44.816057: saving checkpoint...\n",
            "2019-12-17 12:20:45.294186: done, saving took 0.53 seconds\n",
            "2019-12-17 12:20:45.301537: New best epoch (train loss MA): -0.5635\n",
            "2019-12-17 12:20:45.301692: Patience: 0/50\n",
            "2019-12-17 12:20:45.301800: This epoch took 588.362063 s\n",
            "\n",
            "2019-12-17 12:20:45.301898: \n",
            "epoch:  2\n",
            "2019-12-17 12:30:00.410808: train loss : -0.9006\n",
            "2019-12-17 12:30:32.561251: val loss (train=False): -0.9179\n",
            "2019-12-17 12:30:32.561762: Val glob dc per class: [0.9586274461065156]\n",
            "2019-12-17 12:30:33.728878: lr is now (scheduler) 0.0003\n",
            "2019-12-17 12:30:33.729126: current best_val_eval_criterion_MA is 0.93760\n",
            "2019-12-17 12:30:33.729224: current val_eval_criterion_MA is 0.9397\n",
            "2019-12-17 12:30:33.729326: saving best epoch checkpoint...\n",
            "2019-12-17 12:30:33.757234: saving checkpoint...\n",
            "2019-12-17 12:30:34.252355: done, saving took 0.52 seconds\n",
            "2019-12-17 12:30:34.257361: New best epoch (train loss MA): -0.5871\n",
            "2019-12-17 12:30:34.257527: Patience: 0/50\n",
            "2019-12-17 12:30:34.257636: This epoch took 587.259508 s\n",
            "\n",
            "2019-12-17 12:30:34.257737: \n",
            "epoch:  3\n",
            "2019-12-17 12:39:49.281910: train loss : -0.9198\n",
            "2019-12-17 12:40:21.472937: val loss (train=False): -0.9397\n",
            "2019-12-17 12:40:21.473543: Val glob dc per class: [0.9684955344850886]\n",
            "2019-12-17 12:40:22.624635: lr is now (scheduler) 0.0003\n",
            "2019-12-17 12:40:22.624939: current best_val_eval_criterion_MA is 0.93970\n",
            "2019-12-17 12:40:22.625070: current val_eval_criterion_MA is 0.9426\n",
            "2019-12-17 12:40:22.625184: saving best epoch checkpoint...\n",
            "2019-12-17 12:40:22.654062: saving checkpoint...\n",
            "2019-12-17 12:40:23.144782: done, saving took 0.52 seconds\n",
            "2019-12-17 12:40:23.150454: New best epoch (train loss MA): -0.6104\n",
            "2019-12-17 12:40:23.150589: Patience: 0/50\n",
            "2019-12-17 12:40:23.150692: This epoch took 587.215362 s\n",
            "\n",
            "2019-12-17 12:40:23.150787: \n",
            "epoch:  4\n",
            "2019-12-17 12:49:37.804488: train loss : -0.8852\n",
            "2019-12-17 12:50:09.979805: val loss (train=False): -0.9294\n",
            "2019-12-17 12:50:09.980394: Val glob dc per class: [0.9599677453232529]\n",
            "2019-12-17 12:50:11.117163: lr is now (scheduler) 0.0003\n",
            "2019-12-17 12:50:11.117402: current best_val_eval_criterion_MA is 0.94260\n",
            "2019-12-17 12:50:11.117499: current val_eval_criterion_MA is 0.9444\n",
            "2019-12-17 12:50:11.117589: saving best epoch checkpoint...\n",
            "2019-12-17 12:50:11.144776: saving checkpoint...\n",
            "2019-12-17 12:50:11.632704: done, saving took 0.51 seconds\n",
            "2019-12-17 12:50:11.636672: New best epoch (train loss MA): -0.6297\n",
            "2019-12-17 12:50:11.636807: Patience: 0/50\n",
            "2019-12-17 12:50:11.636912: This epoch took 586.829169 s\n",
            "\n",
            "2019-12-17 12:50:11.637002: \n",
            "epoch:  5\n",
            "2019-12-17 12:59:26.775065: train loss : -0.9253\n",
            "2019-12-17 12:59:58.897472: val loss (train=False): -0.9476\n",
            "2019-12-17 12:59:58.897991: Val glob dc per class: [0.9697610775573394]\n",
            "2019-12-17 13:00:00.034209: lr is now (scheduler) 0.0003\n",
            "2019-12-17 13:00:00.034456: current best_val_eval_criterion_MA is 0.94440\n",
            "2019-12-17 13:00:00.034569: current val_eval_criterion_MA is 0.9469\n",
            "2019-12-17 13:00:00.034668: saving best epoch checkpoint...\n",
            "2019-12-17 13:00:00.062302: saving checkpoint...\n",
            "2019-12-17 13:00:00.570113: done, saving took 0.54 seconds\n",
            "2019-12-17 13:00:00.574039: New best epoch (train loss MA): -0.6503\n",
            "2019-12-17 13:00:00.574182: Patience: 0/50\n",
            "2019-12-17 13:00:00.574283: This epoch took 587.260567 s\n",
            "\n",
            "2019-12-17 13:00:00.574357: \n",
            "epoch:  6\n",
            "2019-12-17 13:09:15.889359: train loss : -0.9430\n",
            "2019-12-17 13:09:48.038667: val loss (train=False): -0.9532\n",
            "2019-12-17 13:09:48.039250: Val glob dc per class: [0.972367877426264]\n",
            "2019-12-17 13:09:49.171719: lr is now (scheduler) 0.0003\n",
            "2019-12-17 13:09:49.171941: current best_val_eval_criterion_MA is 0.94690\n",
            "2019-12-17 13:09:49.172040: current val_eval_criterion_MA is 0.9494\n",
            "2019-12-17 13:09:49.172154: saving best epoch checkpoint...\n",
            "2019-12-17 13:09:49.198180: saving checkpoint...\n",
            "2019-12-17 13:09:49.707017: done, saving took 0.53 seconds\n",
            "2019-12-17 13:09:49.710635: New best epoch (train loss MA): -0.6708\n",
            "2019-12-17 13:09:49.710747: Patience: 0/50\n",
            "2019-12-17 13:09:49.710843: This epoch took 587.464435 s\n",
            "\n",
            "2019-12-17 13:09:49.710936: \n",
            "epoch:  7\n",
            "2019-12-17 13:19:05.161790: train loss : -0.9482\n",
            "2019-12-17 13:19:37.423283: val loss (train=False): -0.9562\n",
            "2019-12-17 13:19:37.423899: Val glob dc per class: [0.9738332521454803]\n",
            "2019-12-17 13:19:38.558142: lr is now (scheduler) 0.0003\n",
            "2019-12-17 13:19:38.558395: current best_val_eval_criterion_MA is 0.94940\n",
            "2019-12-17 13:19:38.558504: current val_eval_criterion_MA is 0.9519\n",
            "2019-12-17 13:19:38.558604: saving best epoch checkpoint...\n",
            "2019-12-17 13:19:38.586103: saving checkpoint...\n",
            "2019-12-17 13:19:39.085155: done, saving took 0.53 seconds\n",
            "2019-12-17 13:19:39.089649: New best epoch (train loss MA): -0.6903\n",
            "2019-12-17 13:19:39.089784: Patience: 0/50\n",
            "2019-12-17 13:19:39.089884: This epoch took 587.712548 s\n",
            "\n",
            "2019-12-17 13:19:39.089991: \n",
            "epoch:  8\n",
            "2019-12-17 13:28:54.723813: train loss : -0.9440\n",
            "2019-12-17 13:29:26.993066: val loss (train=False): -0.9520\n",
            "2019-12-17 13:29:26.994292: Val glob dc per class: [0.9711499698933908]\n",
            "2019-12-17 13:29:28.130810: lr is now (scheduler) 0.0003\n",
            "2019-12-17 13:29:28.131021: current best_val_eval_criterion_MA is 0.95190\n",
            "2019-12-17 13:29:28.131118: current val_eval_criterion_MA is 0.9538\n",
            "2019-12-17 13:29:28.131185: saving best epoch checkpoint...\n",
            "2019-12-17 13:29:28.163261: saving checkpoint...\n",
            "2019-12-17 13:29:28.685534: done, saving took 0.55 seconds\n",
            "2019-12-17 13:29:28.690693: New best epoch (train loss MA): -0.7080\n",
            "2019-12-17 13:29:28.690843: Patience: 0/50\n",
            "2019-12-17 13:29:28.690931: This epoch took 587.903343 s\n",
            "\n",
            "2019-12-17 13:29:28.691006: \n",
            "epoch:  9\n",
            "2019-12-17 13:38:43.310758: train loss : -0.9505\n",
            "2019-12-17 13:39:15.504927: val loss (train=False): -0.9609\n",
            "2019-12-17 13:39:15.505469: Val glob dc per class: [0.9763223917826066]\n",
            "2019-12-17 13:39:16.625292: lr is now (scheduler) 0.0003\n",
            "2019-12-17 13:39:16.625532: current best_val_eval_criterion_MA is 0.95380\n",
            "2019-12-17 13:39:16.625623: current val_eval_criterion_MA is 0.9561\n",
            "2019-12-17 13:39:16.625710: saving best epoch checkpoint...\n",
            "2019-12-17 13:39:16.651231: saving checkpoint...\n",
            "2019-12-17 13:39:17.091308: done, saving took 0.47 seconds\n",
            "2019-12-17 13:39:17.095791: New best epoch (train loss MA): -0.7250\n",
            "2019-12-17 13:39:17.095911: Patience: 0/50\n",
            "2019-12-17 13:39:17.096012: This epoch took 586.814072 s\n",
            "\n",
            "2019-12-17 13:39:17.096125: \n",
            "epoch:  10\n",
            "2019-12-17 13:48:32.850780: train loss : -0.9563\n",
            "2019-12-17 13:49:05.011521: val loss (train=False): -0.9648\n",
            "2019-12-17 13:49:05.012300: Val glob dc per class: [0.9785086114529754]\n",
            "2019-12-17 13:49:06.097888: lr is now (scheduler) 0.0003\n",
            "2019-12-17 13:49:06.098126: current best_val_eval_criterion_MA is 0.95610\n",
            "2019-12-17 13:49:06.098222: current val_eval_criterion_MA is 0.9583\n",
            "2019-12-17 13:49:06.098312: saving best epoch checkpoint...\n",
            "2019-12-17 13:49:06.124904: saving checkpoint...\n",
            "2019-12-17 13:49:06.641702: done, saving took 0.54 seconds\n",
            "2019-12-17 13:49:06.654864: New best epoch (train loss MA): -0.7412\n",
            "2019-12-17 13:49:06.654996: Patience: 0/50\n",
            "2019-12-17 13:49:06.655868: This epoch took 587.915672 s\n",
            "\n",
            "2019-12-17 13:49:06.655964: \n",
            "epoch:  11\n",
            "2019-12-17 13:58:21.840739: train loss : -0.9574\n",
            "2019-12-17 13:58:54.016785: val loss (train=False): -0.9588\n",
            "2019-12-17 13:58:54.017336: Val glob dc per class: [0.9746795206926118]\n",
            "2019-12-17 13:58:55.123446: lr is now (scheduler) 0.0003\n",
            "2019-12-17 13:58:55.123690: current best_val_eval_criterion_MA is 0.95830\n",
            "2019-12-17 13:58:55.123814: current val_eval_criterion_MA is 0.9599\n",
            "2019-12-17 13:58:55.123945: saving best epoch checkpoint...\n",
            "2019-12-17 13:58:55.143987: saving checkpoint...\n",
            "2019-12-17 13:58:55.563001: done, saving took 0.44 seconds\n",
            "2019-12-17 13:58:55.566110: New best epoch (train loss MA): -0.7563\n",
            "2019-12-17 13:58:55.566227: Patience: 0/50\n",
            "2019-12-17 13:58:55.566315: This epoch took 587.361003 s\n",
            "\n",
            "2019-12-17 13:58:55.566387: \n",
            "epoch:  12\n",
            "2019-12-17 14:08:11.018781: train loss : -0.9584\n",
            "2019-12-17 14:08:43.236397: val loss (train=False): -0.9658\n",
            "2019-12-17 14:08:43.237109: Val glob dc per class: [0.9788314374228183]\n",
            "2019-12-17 14:08:44.332969: lr is now (scheduler) 0.0003\n",
            "2019-12-17 14:08:44.333225: current best_val_eval_criterion_MA is 0.95990\n",
            "2019-12-17 14:08:44.333321: current val_eval_criterion_MA is 0.9618\n",
            "2019-12-17 14:08:44.333416: saving best epoch checkpoint...\n",
            "2019-12-17 14:08:44.352671: saving checkpoint...\n",
            "2019-12-17 14:08:44.847170: done, saving took 0.51 seconds\n",
            "2019-12-17 14:08:44.853550: New best epoch (train loss MA): -0.7705\n",
            "2019-12-17 14:08:44.853677: Patience: 0/50\n",
            "2019-12-17 14:08:44.853757: This epoch took 587.670200 s\n",
            "\n",
            "2019-12-17 14:08:44.853827: \n",
            "epoch:  13\n",
            "2019-12-17 14:18:00.079682: train loss : -0.9559\n",
            "2019-12-17 14:18:32.217939: val loss (train=False): -0.9651\n",
            "2019-12-17 14:18:32.218532: Val glob dc per class: [0.9783789774857184]\n",
            "2019-12-17 14:18:33.322608: lr is now (scheduler) 0.0003\n",
            "2019-12-17 14:18:33.322869: current best_val_eval_criterion_MA is 0.96180\n",
            "2019-12-17 14:18:33.322963: current val_eval_criterion_MA is 0.9635\n",
            "2019-12-17 14:18:33.323091: saving best epoch checkpoint...\n",
            "2019-12-17 14:18:33.342405: saving checkpoint...\n",
            "2019-12-17 14:18:33.859587: done, saving took 0.54 seconds\n",
            "2019-12-17 14:18:33.864940: New best epoch (train loss MA): -0.7834\n",
            "2019-12-17 14:18:33.865098: Patience: 0/50\n",
            "2019-12-17 14:18:33.865182: This epoch took 587.364355 s\n",
            "\n",
            "2019-12-17 14:18:33.865261: \n",
            "epoch:  14\n",
            "2019-12-17 14:27:49.179696: train loss : -0.9592\n",
            "2019-12-17 14:28:21.341068: val loss (train=False): -0.9653\n",
            "2019-12-17 14:28:21.341642: Val glob dc per class: [0.9783566085281478]\n",
            "2019-12-17 14:28:22.439946: lr is now (scheduler) 0.0003\n",
            "2019-12-17 14:28:22.440227: current best_val_eval_criterion_MA is 0.96350\n",
            "2019-12-17 14:28:22.440326: current val_eval_criterion_MA is 0.9650\n",
            "2019-12-17 14:28:22.440415: saving best epoch checkpoint...\n",
            "2019-12-17 14:28:22.458447: saving checkpoint...\n",
            "2019-12-17 14:28:22.956480: done, saving took 0.52 seconds\n",
            "2019-12-17 14:28:22.962877: New best epoch (train loss MA): -0.7957\n",
            "2019-12-17 14:28:22.963044: Patience: 0/50\n",
            "2019-12-17 14:28:22.963152: This epoch took 587.476018 s\n",
            "\n",
            "2019-12-17 14:28:22.963234: \n",
            "epoch:  15\n",
            "2019-12-17 14:37:38.617186: train loss : -0.9613\n",
            "2019-12-17 14:38:10.790302: val loss (train=False): -0.9686\n",
            "2019-12-17 14:38:10.790847: Val glob dc per class: [0.9803477950486674]\n",
            "2019-12-17 14:38:11.885149: lr is now (scheduler) 0.0003\n",
            "2019-12-17 14:38:11.885363: current best_val_eval_criterion_MA is 0.96500\n",
            "2019-12-17 14:38:11.885439: current val_eval_criterion_MA is 0.9665\n",
            "2019-12-17 14:38:11.885529: saving best epoch checkpoint...\n",
            "2019-12-17 14:38:11.904508: saving checkpoint...\n",
            "2019-12-17 14:38:12.432174: done, saving took 0.55 seconds\n",
            "2019-12-17 14:38:12.436284: New best epoch (train loss MA): -0.8073\n",
            "2019-12-17 14:38:12.436412: Patience: 0/50\n",
            "2019-12-17 14:38:12.436490: This epoch took 587.827268 s\n",
            "\n",
            "2019-12-17 14:38:12.436557: \n",
            "epoch:  16\n",
            "2019-12-17 14:47:27.633249: train loss : -0.9630\n",
            "2019-12-17 14:47:59.828856: val loss (train=False): -0.9698\n",
            "2019-12-17 14:47:59.829443: Val glob dc per class: [0.9813459506526513]\n",
            "2019-12-17 14:48:00.933618: lr is now (scheduler) 0.0003\n",
            "2019-12-17 14:48:00.933851: current best_val_eval_criterion_MA is 0.96650\n",
            "2019-12-17 14:48:00.933930: current val_eval_criterion_MA is 0.9680\n",
            "2019-12-17 14:48:00.934021: saving best epoch checkpoint...\n",
            "2019-12-17 14:48:00.953053: saving checkpoint...\n",
            "2019-12-17 14:48:01.474533: done, saving took 0.54 seconds\n",
            "2019-12-17 14:48:01.480370: New best epoch (train loss MA): -0.8182\n",
            "2019-12-17 14:48:01.480512: Patience: 0/50\n",
            "2019-12-17 14:48:01.480591: This epoch took 587.392503 s\n",
            "\n",
            "2019-12-17 14:48:01.480660: \n",
            "epoch:  17\n",
            "2019-12-17 14:57:16.971519: train loss : -0.9632\n",
            "2019-12-17 14:57:49.151057: val loss (train=False): -0.9705\n",
            "2019-12-17 14:57:49.151640: Val glob dc per class: [0.9813608466379665]\n",
            "2019-12-17 14:57:50.234920: lr is now (scheduler) 0.0003\n",
            "2019-12-17 14:57:50.235165: current best_val_eval_criterion_MA is 0.96800\n",
            "2019-12-17 14:57:50.235264: current val_eval_criterion_MA is 0.9693\n",
            "2019-12-17 14:57:50.235353: saving best epoch checkpoint...\n",
            "2019-12-17 14:57:50.253912: saving checkpoint...\n",
            "2019-12-17 14:57:51.339070: done, saving took 1.10 seconds\n",
            "2019-12-17 14:57:51.343826: New best epoch (train loss MA): -0.8284\n",
            "2019-12-17 14:57:51.343935: Patience: 0/50\n",
            "2019-12-17 14:57:51.344041: This epoch took 587.670607 s\n",
            "\n",
            "2019-12-17 14:57:51.344156: \n",
            "epoch:  18\n",
            "2019-12-17 15:07:07.164168: train loss : -0.9638\n",
            "2019-12-17 15:07:39.402372: val loss (train=False): -0.9709\n",
            "2019-12-17 15:07:39.402991: Val glob dc per class: [0.9815984227630807]\n",
            "2019-12-17 15:07:40.503558: lr is now (scheduler) 0.0003\n",
            "2019-12-17 15:07:40.503786: current best_val_eval_criterion_MA is 0.96930\n",
            "2019-12-17 15:07:40.503877: current val_eval_criterion_MA is 0.9706\n",
            "2019-12-17 15:07:40.503942: saving best epoch checkpoint...\n",
            "2019-12-17 15:07:40.523053: saving checkpoint...\n",
            "2019-12-17 15:07:41.058139: done, saving took 0.55 seconds\n",
            "2019-12-17 15:07:41.063921: New best epoch (train loss MA): -0.8379\n",
            "2019-12-17 15:07:41.064026: Patience: 0/50\n",
            "2019-12-17 15:07:41.064155: This epoch took 588.058393 s\n",
            "\n",
            "2019-12-17 15:07:41.064244: \n",
            "epoch:  19\n",
            "2019-12-17 15:17:00.364309: train loss : -0.9652\n",
            "2019-12-17 15:17:33.053935: val loss (train=False): -0.9717\n",
            "2019-12-17 15:17:33.055151: Val glob dc per class: [0.9821535589571947]\n",
            "2019-12-17 15:17:34.140456: lr is now (scheduler) 0.0003\n",
            "2019-12-17 15:17:34.140702: current best_val_eval_criterion_MA is 0.97060\n",
            "2019-12-17 15:17:34.140782: current val_eval_criterion_MA is 0.9717\n",
            "2019-12-17 15:17:34.140855: saving best epoch checkpoint...\n",
            "2019-12-17 15:17:34.159667: saving checkpoint...\n",
            "2019-12-17 15:17:34.723066: done, saving took 0.58 seconds\n",
            "2019-12-17 15:17:34.726063: New best epoch (train loss MA): -0.8468\n",
            "2019-12-17 15:17:34.726184: Patience: 0/50\n",
            "2019-12-17 15:17:34.726259: This epoch took 591.989909 s\n",
            "\n",
            "2019-12-17 15:17:34.726328: \n",
            "epoch:  20\n",
            "2019-12-17 15:27:00.973567: train loss : -0.9661\n",
            "2019-12-17 15:27:33.693528: val loss (train=False): -0.9732\n",
            "2019-12-17 15:27:33.694201: Val glob dc per class: [0.9831984939509321]\n",
            "2019-12-17 15:27:34.799709: lr is now (scheduler) 0.0003\n",
            "2019-12-17 15:27:34.799933: current best_val_eval_criterion_MA is 0.97170\n",
            "2019-12-17 15:27:34.800030: current val_eval_criterion_MA is 0.9729\n",
            "2019-12-17 15:27:34.800140: saving best epoch checkpoint...\n",
            "2019-12-17 15:27:34.819111: saving checkpoint...\n",
            "2019-12-17 15:27:35.341655: done, saving took 0.54 seconds\n",
            "2019-12-17 15:27:35.345630: New best epoch (train loss MA): -0.8551\n",
            "2019-12-17 15:27:35.345775: Patience: 0/50\n",
            "2019-12-17 15:27:35.345879: This epoch took 598.967412 s\n",
            "\n",
            "2019-12-17 15:27:35.345989: \n",
            "epoch:  21\n",
            "2019-12-17 15:37:00.434520: train loss : -0.9666\n",
            "2019-12-17 15:37:32.744866: val loss (train=False): -0.9732\n",
            "2019-12-17 15:37:32.745605: Val glob dc per class: [0.9831250698779391]\n",
            "2019-12-17 15:37:33.835258: lr is now (scheduler) 0.0003\n",
            "2019-12-17 15:37:33.835474: current best_val_eval_criterion_MA is 0.97290\n",
            "2019-12-17 15:37:33.835557: current val_eval_criterion_MA is 0.9739\n",
            "2019-12-17 15:37:33.835639: saving best epoch checkpoint...\n",
            "2019-12-17 15:37:33.862364: saving checkpoint...\n",
            "2019-12-17 15:37:34.500103: done, saving took 0.66 seconds\n",
            "2019-12-17 15:37:34.516629: New best epoch (train loss MA): -0.8629\n",
            "2019-12-17 15:37:34.516785: Patience: 0/50\n",
            "2019-12-17 15:37:34.516862: This epoch took 597.399178 s\n",
            "\n",
            "2019-12-17 15:37:34.516942: \n",
            "epoch:  22\n",
            "2019-12-17 15:46:50.991116: train loss : -0.9628\n",
            "2019-12-17 15:47:23.168337: val loss (train=False): -0.9669\n",
            "2019-12-17 15:47:23.168831: Val glob dc per class: [0.9788638426802992]\n",
            "2019-12-17 15:47:24.228610: lr is now (scheduler) 0.0003\n",
            "2019-12-17 15:47:24.228822: current best_val_eval_criterion_MA is 0.97390\n",
            "2019-12-17 15:47:24.228925: current val_eval_criterion_MA is 0.9744\n",
            "2019-12-17 15:47:24.229015: saving best epoch checkpoint...\n",
            "2019-12-17 15:47:24.247827: saving checkpoint...\n",
            "2019-12-17 15:47:24.775547: done, saving took 0.55 seconds\n",
            "2019-12-17 15:47:24.779287: New best epoch (train loss MA): -0.8699\n",
            "2019-12-17 15:47:24.779495: Patience: 0/50\n",
            "2019-12-17 15:47:24.779625: This epoch took 588.651561 s\n",
            "\n",
            "2019-12-17 15:47:24.779718: \n",
            "epoch:  23\n",
            "2019-12-17 15:56:40.264707: train loss : -0.9635\n",
            "2019-12-17 15:57:12.441578: val loss (train=False): -0.9736\n",
            "2019-12-17 15:57:12.442130: Val glob dc per class: [0.9835357150003669]\n",
            "2019-12-17 15:57:13.491493: lr is now (scheduler) 0.0003\n",
            "2019-12-17 15:57:13.491723: current best_val_eval_criterion_MA is 0.97440\n",
            "2019-12-17 15:57:13.491803: current val_eval_criterion_MA is 0.9753\n",
            "2019-12-17 15:57:13.491878: saving best epoch checkpoint...\n",
            "2019-12-17 15:57:13.511270: saving checkpoint...\n",
            "2019-12-17 15:57:14.128130: done, saving took 0.64 seconds\n",
            "2019-12-17 15:57:14.131639: New best epoch (train loss MA): -0.8765\n",
            "2019-12-17 15:57:14.131763: Patience: 0/50\n",
            "2019-12-17 15:57:14.131879: This epoch took 587.662007 s\n",
            "\n",
            "2019-12-17 15:57:14.131959: \n",
            "epoch:  24\n",
            "2019-12-17 16:06:29.711987: train loss : -0.9666\n",
            "2019-12-17 16:07:01.895673: val loss (train=False): -0.9692\n",
            "2019-12-17 16:07:01.896235: Val glob dc per class: [0.9805842653667559]\n",
            "2019-12-17 16:07:02.977681: lr is now (scheduler) 0.0003\n",
            "2019-12-17 16:07:02.977886: current best_val_eval_criterion_MA is 0.97530\n",
            "2019-12-17 16:07:02.977964: current val_eval_criterion_MA is 0.9758\n",
            "2019-12-17 16:07:02.978026: saving best epoch checkpoint...\n",
            "2019-12-17 16:07:02.996916: saving checkpoint...\n",
            "2019-12-17 16:07:03.500355: done, saving took 0.52 seconds\n",
            "2019-12-17 16:07:03.505124: New best epoch (train loss MA): -0.8828\n",
            "2019-12-17 16:07:03.505245: Patience: 0/50\n",
            "2019-12-17 16:07:03.505323: This epoch took 587.763883 s\n",
            "\n",
            "2019-12-17 16:07:03.505391: \n",
            "epoch:  25\n",
            "Process Process-20:\n",
            "Process Process-24:\n",
            "Process Process-19:\n",
            "Process Process-10:\n",
            "Process Process-13:\n",
            "Process Process-22:\n",
            "Process Process-14:\n",
            "Process Process-18:\n",
            "Process Process-21:\n",
            "Process Process-12:\n",
            "Process Process-23:\n",
            "Process Process-11:\n",
            "Process Process-17:\n",
            "Process Process-15:\n",
            "Process Process-9:\n",
            "Process Process-26:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Process Process-25:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Process Process-16:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 50, in producer\n",
            "    item = transform(**item)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/transforms/abstract_transforms.py\", line 88, in __call__\n",
            "    data_dict = t(**data_dict)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/transforms/spatial_transforms.py\", line 350, in __call__\n",
            "    p_rot_per_sample=self.p_rot_per_sample)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/augmentations/spatial_transformations.py\", line 221, in augment_spatial\n",
            "    coords = elastic_deform_coordinates(coords, a, s)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/augmentations/utils.py\", line 79, in elastic_deform_coordinates\n",
            "    gaussian_filter((np.random.random(coordinates.shape[1:]) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/filters.py\", line 299, in gaussian_filter\n",
            "    mode, cval, truncate)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/filters.py\", line 217, in gaussian_filter1d\n",
            "    return correlate1d(input, weights, axis, output, mode, cval, 0)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/filters.py\", line 95, in correlate1d\n",
            "    origin)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"run/run_training.py\", line 103, in <module>\n",
            "    trainer.run_training()\n",
            "  File \"/content/nnUNet/nnunet/training/network_training/nnUNetTrainer.py\", line 275, in run_training\n",
            "    super(nnUNetTrainer, self).run_training()\n",
            "  File \"/content/nnUNet/nnunet/training/network_training/network_trainer.py\", line 351, in run_training\n",
            "    l = self.run_iteration(self.tr_gen, True)\n",
            "  File \"/content/nnUNet/nnunet/training/network_training/network_trainer.py\", line 534, in run_iteration\n",
            "    l = self.loss(output, target)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/nnUNet/nnunet/training/loss_functions/dice_loss.py\", line 122, in forward\n",
            "    dc_loss = self.dc(net_output, target)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/nnUNet/nnunet/training/loss_functions/dice_loss.py\", line 100, in forward\n",
            "    tp, fp, fn = get_tp_fp_fn(x, y, axes, loss_mask, self.square)\n",
            "  File \"/content/nnUNet/nnunet/training/loss_functions/dice_loss.py\", line 51, in get_tp_fp_fn\n",
            "    y_onehot = y_onehot.cuda(net_output.device.index)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX9rFG6So86E",
        "colab_type": "text"
      },
      "source": [
        "## Save the checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmOkUvRpod6E",
        "colab_type": "code",
        "outputId": "3088c6c5-1eb7-4e13-eb24-552beef74164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "import colab_transfer\n",
        "\n",
        "colab_transfer.copy_folder_structure(\n",
        "    '/content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/',\n",
        "    '/content/drive/My Drive/nnUNet_results/'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all', '/content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/plans.pkl']\n",
            "['/content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/plans.pkl']\n",
            "/content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all\n",
            "all/\n",
            "debug.json\n",
            "Copying debug.json\n",
            "all/\n",
            "training_log_2019_12_17_12_00_45.txt\n",
            "Copying training_log_2019_12_17_12_00_45.txt\n",
            "all/\n",
            "model_latest.model\n",
            "Copying model_latest.model\n",
            "all/\n",
            "training_log_2019_12_22_17_38_11.txt\n",
            "Copying training_log_2019_12_22_17_38_11.txt\n",
            "all/\n",
            "model_latest.model.pkl\n",
            "Copying model_latest.model.pkl\n",
            "all/\n",
            "model_best.model.pkl\n",
            "Copying model_best.model.pkl\n",
            "all/\n",
            "progress.png\n",
            "Copying progress.png\n",
            "all/\n",
            "model_best.model\n",
            "Copying model_best.model\n",
            "/content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/plans.pkl\n",
            "plans.pkl\n",
            "Copying plans.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKllmvXqrG3T",
        "colab_type": "text"
      },
      "source": [
        "## Load the checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AUH-BL1rGQu",
        "colab_type": "code",
        "outputId": "ce8a852f-d432-4e6f-c9c1-e0cb373cd750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "import colab_transfer\n",
        "\n",
        "colab_transfer.copy_folder_structure(\n",
        "    '/content/drive/My Drive/nnUNet_results/',\n",
        "    '/content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/',\n",
        ")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files: ['/content/drive/My Drive/nnUNet_results/plans.pkl']\n",
            "Folders: {'/content/drive/My Drive/nnUNet_results/all'}\n",
            "Copying /content/drive/My Drive/nnUNet_results/plans.pkl to /content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/plans.pkl\n",
            "Copying /content/drive/My Drive/nnUNet_results/all/debug.json to /content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all/debug.json\n",
            "Copying /content/drive/My Drive/nnUNet_results/all/training_log_2019_12_17_12_00_45.txt to /content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all/training_log_2019_12_17_12_00_45.txt\n",
            "Copying /content/drive/My Drive/nnUNet_results/all/model_latest.model to /content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all/model_latest.model\n",
            "Copying /content/drive/My Drive/nnUNet_results/all/training_log_2019_12_22_17_38_11.txt to /content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all/training_log_2019_12_22_17_38_11.txt\n",
            "Copying /content/drive/My Drive/nnUNet_results/all/model_latest.model.pkl to /content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all/model_latest.model.pkl\n",
            "Copying /content/drive/My Drive/nnUNet_results/all/model_best.model.pkl to /content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all/model_best.model.pkl\n",
            "Copying /content/drive/My Drive/nnUNet_results/all/progress.png to /content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all/progress.png\n",
            "Copying /content/drive/My Drive/nnUNet_results/all/model_best.model to /content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all/model_best.model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AH9gMAnpQY0",
        "colab_type": "text"
      },
      "source": [
        "## Continue training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i081dBIZpHdf",
        "colab_type": "code",
        "outputId": "933d96db-c29e-4783-bd42-8dfe49b7e6f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!OMP_NUM_THREADS=1 python run/run_training.py 3d_fullres nnUNetTrainer Task00_MY_DATASET all --ndet -c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, Fabian, et al. \"nnU-Net: Breaking the Spell on Successful Medical Image Segmentation.\" arXiv preprint arXiv:1904.08128 (2019).\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "###############################################\n",
            "I am running the following nnUNet: 3d_fullres\n",
            "My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer'>\n",
            "For that I will be using the following configuration:\n",
            "num_classes:  1\n",
            "modalities:  {0: 'CT'}\n",
            "use_mask_for_norm OrderedDict([(0, False)])\n",
            "keep_only_largest_region OrderedDict([((1,), True)])\n",
            "min_region_size_per_class OrderedDict([(1, 699477.76)])\n",
            "min_size_per_class OrderedDict([(1, 699477.76)])\n",
            "normalization_schemes OrderedDict([(0, 'CT')])\n",
            "stages...\n",
            "\n",
            "stage:  0\n",
            "{'batch_size': 2, 'num_pool_per_axis': [3, 4, 4], 'patch_size': array([40, 96, 96]), 'median_patient_size_in_voxels': array([38, 94, 94]), 'current_spacing': array([4., 4., 4.]), 'original_spacing': array([4., 4., 4.]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
            "\n",
            "I am using stage 0 from these plans\n",
            "I am using sample dice + CE loss\n",
            "\n",
            "I am using data from this folder:  /content/nnUNet_preprocessed/Task00_MY_DATASET/nnUNet\n",
            "###############################################\n",
            "2019-12-22 17:38:11.399477: Creating new split...\n",
            "2019-12-22 17:38:11.400787: unpacking dataset\n",
            "2019-12-22 17:38:11.544458: done\n",
            "2019-12-22 17:38:20.887421: loading checkpoint /content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all/model_best.model train= True\n",
            "2019-12-22 17:38:21.070985: Unable to plot network architecture:\n",
            "2019-12-22 17:38:21.071126: No module named 'hiddenlayer'\n",
            "2019-12-22 17:38:21.071286: \n",
            "epoch:  25\n",
            "gpu 0\n",
            "2019-12-22 17:40:55.111789: train loss : -0.9656\n",
            "gpu 0\n",
            "2019-12-22 17:41:02.803962: val loss (train=False): -0.9736\n",
            "2019-12-22 17:41:02.804585: Val glob dc per class: [0.9833231614837482]\n",
            "2019-12-22 17:41:03.950175: lr is now (scheduler) 0.0003\n",
            "2019-12-22 17:41:03.950394: current best_val_eval_criterion_MA is 0.98330\n",
            "2019-12-22 17:41:03.950488: current val_eval_criterion_MA is 0.9833\n",
            "2019-12-22 17:41:03.950589: No improvement: current train MA -0.9656, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 17:41:03.950678: Patience: 0/50\n",
            "2019-12-22 17:41:03.950746: This epoch took 161.732864 s\n",
            "\n",
            "2019-12-22 17:41:03.950820: \n",
            "epoch:  26\n",
            "2019-12-22 17:43:25.975457: train loss : -0.9679\n",
            "2019-12-22 17:43:32.835982: val loss (train=False): -0.9744\n",
            "2019-12-22 17:43:32.836550: Val glob dc per class: [0.9840651710792504]\n",
            "2019-12-22 17:43:33.958734: lr is now (scheduler) 0.0003\n",
            "2019-12-22 17:43:33.958953: current best_val_eval_criterion_MA is 0.98330\n",
            "2019-12-22 17:43:33.959077: current val_eval_criterion_MA is 0.9834\n",
            "2019-12-22 17:43:33.959173: saving best epoch checkpoint...\n",
            "2019-12-22 17:43:34.006675: saving checkpoint...\n",
            "2019-12-22 17:43:34.383165: done, saving took 0.42 seconds\n",
            "2019-12-22 17:43:34.391262: No improvement: current train MA -0.9658, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 17:43:34.391409: Patience: 1/50\n",
            "2019-12-22 17:43:34.391490: This epoch took 148.885376 s\n",
            "\n",
            "2019-12-22 17:43:34.391558: \n",
            "epoch:  27\n",
            "2019-12-22 17:45:56.376628: train loss : -0.9675\n",
            "2019-12-22 17:46:03.266932: val loss (train=False): -0.9752\n",
            "2019-12-22 17:46:03.267394: Val glob dc per class: [0.9843323704060344]\n",
            "2019-12-22 17:46:04.331665: lr is now (scheduler) 0.0003\n",
            "2019-12-22 17:46:04.331886: current best_val_eval_criterion_MA is 0.98340\n",
            "2019-12-22 17:46:04.331983: current val_eval_criterion_MA is 0.9835\n",
            "2019-12-22 17:46:04.332080: saving best epoch checkpoint...\n",
            "2019-12-22 17:46:04.356392: saving checkpoint...\n",
            "2019-12-22 17:46:04.813409: done, saving took 0.48 seconds\n",
            "2019-12-22 17:46:04.817401: No improvement: current train MA -0.9659, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 17:46:04.817529: Patience: 2/50\n",
            "2019-12-22 17:46:04.817608: This epoch took 148.875531 s\n",
            "\n",
            "2019-12-22 17:46:04.817679: \n",
            "epoch:  28\n",
            "2019-12-22 17:48:27.340663: train loss : -0.9652\n",
            "2019-12-22 17:48:34.190587: val loss (train=False): -0.9478\n",
            "2019-12-22 17:48:34.191163: Val glob dc per class: [0.966694303880542]\n",
            "2019-12-22 17:48:35.266578: lr is now (scheduler) 0.0003\n",
            "2019-12-22 17:48:35.266772: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 17:48:35.266850: current val_eval_criterion_MA is 0.9818\n",
            "2019-12-22 17:48:35.266923: No improvement: current train MA -0.9659, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 17:48:35.266988: Patience: 3/50\n",
            "2019-12-22 17:48:35.267074: This epoch took 149.373068 s\n",
            "\n",
            "2019-12-22 17:48:35.267140: \n",
            "epoch:  29\n",
            "2019-12-22 17:50:57.424059: train loss : -0.9620\n",
            "2019-12-22 17:51:04.233402: val loss (train=False): -0.9724\n",
            "2019-12-22 17:51:04.233857: Val glob dc per class: [0.9824782059533816]\n",
            "2019-12-22 17:51:05.308488: lr is now (scheduler) 0.0003\n",
            "2019-12-22 17:51:05.308742: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 17:51:05.308840: current val_eval_criterion_MA is 0.9819\n",
            "2019-12-22 17:51:05.308953: No improvement: current train MA -0.9656, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 17:51:05.309074: Patience: 4/50\n",
            "2019-12-22 17:51:05.309238: This epoch took 148.966398 s\n",
            "\n",
            "2019-12-22 17:51:05.309355: \n",
            "epoch:  30\n",
            "2019-12-22 17:53:27.919101: train loss : -0.8985\n",
            "2019-12-22 17:53:34.782842: val loss (train=False): -0.9407\n",
            "2019-12-22 17:53:34.783339: Val glob dc per class: [0.9625857011959806]\n",
            "2019-12-22 17:53:35.898671: lr is now (scheduler) 0.0003\n",
            "2019-12-22 17:53:35.898874: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 17:53:35.898990: current val_eval_criterion_MA is 0.9799\n",
            "2019-12-22 17:53:35.899126: No improvement: current train MA -0.9609, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 17:53:35.899218: Patience: 5/50\n",
            "2019-12-22 17:53:35.899328: This epoch took 149.473618 s\n",
            "\n",
            "2019-12-22 17:53:35.899427: \n",
            "epoch:  31\n",
            "2019-12-22 17:55:57.822696: train loss : -0.9483\n",
            "2019-12-22 17:56:04.845210: val loss (train=False): -0.9530\n",
            "2019-12-22 17:56:04.845685: Val glob dc per class: [0.9709629926006388]\n",
            "2019-12-22 17:56:05.956204: lr is now (scheduler) 0.0003\n",
            "2019-12-22 17:56:05.956432: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 17:56:05.956535: current val_eval_criterion_MA is 0.9791\n",
            "2019-12-22 17:56:05.956650: No improvement: current train MA -0.9600, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 17:56:05.956744: Patience: 6/50\n",
            "2019-12-22 17:56:05.956849: This epoch took 148.945879 s\n",
            "\n",
            "2019-12-22 17:56:05.956946: \n",
            "epoch:  32\n",
            "2019-12-22 17:58:28.372522: train loss : -0.9570\n",
            "2019-12-22 17:58:35.161154: val loss (train=False): -0.9625\n",
            "2019-12-22 17:58:35.161626: Val glob dc per class: [0.9760299830342573]\n",
            "2019-12-22 17:58:36.286840: lr is now (scheduler) 0.0003\n",
            "2019-12-22 17:58:36.287121: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 17:58:36.287230: current val_eval_criterion_MA is 0.9787\n",
            "2019-12-22 17:58:36.287343: No improvement: current train MA -0.9598, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 17:58:36.287438: Patience: 7/50\n",
            "2019-12-22 17:58:36.287539: This epoch took 149.204316 s\n",
            "\n",
            "2019-12-22 17:58:36.287642: \n",
            "epoch:  33\n",
            "2019-12-22 18:00:59.163095: train loss : -0.9582\n",
            "2019-12-22 18:01:05.892703: val loss (train=False): -0.9673\n",
            "2019-12-22 18:01:05.893224: Val glob dc per class: [0.9790193333506592]\n",
            "2019-12-22 18:01:06.955669: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:01:06.955855: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:01:06.955946: current val_eval_criterion_MA is 0.9788\n",
            "2019-12-22 18:01:06.956032: No improvement: current train MA -0.9597, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:01:06.956100: Patience: 8/50\n",
            "2019-12-22 18:01:06.956159: This epoch took 149.605117 s\n",
            "\n",
            "2019-12-22 18:01:06.956219: \n",
            "epoch:  34\n",
            "2019-12-22 18:03:28.677837: train loss : -0.9633\n",
            "2019-12-22 18:03:35.516345: val loss (train=False): -0.9702\n",
            "2019-12-22 18:03:35.516749: Val glob dc per class: [0.9810398665476303]\n",
            "2019-12-22 18:03:36.595756: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:03:36.595952: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:03:36.596099: current val_eval_criterion_MA is 0.9790\n",
            "2019-12-22 18:03:36.596174: No improvement: current train MA -0.9599, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:03:36.596238: Patience: 9/50\n",
            "2019-12-22 18:03:36.596298: This epoch took 148.560235 s\n",
            "\n",
            "2019-12-22 18:03:36.596358: \n",
            "epoch:  35\n",
            "2019-12-22 18:05:58.612607: train loss : -0.9634\n",
            "2019-12-22 18:06:05.419442: val loss (train=False): -0.9692\n",
            "2019-12-22 18:06:05.419899: Val glob dc per class: [0.9803697828095729]\n",
            "2019-12-22 18:06:06.485847: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:06:06.486080: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:06:06.486173: current val_eval_criterion_MA is 0.9791\n",
            "2019-12-22 18:06:06.486248: No improvement: current train MA -0.9602, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:06:06.486328: Patience: 10/50\n",
            "2019-12-22 18:06:06.486392: This epoch took 148.823236 s\n",
            "\n",
            "2019-12-22 18:06:06.486464: \n",
            "epoch:  36\n",
            "2019-12-22 18:08:27.962718: train loss : -0.9649\n",
            "2019-12-22 18:08:34.748073: val loss (train=False): -0.9716\n",
            "2019-12-22 18:08:34.748531: Val glob dc per class: [0.9820345393385685]\n",
            "2019-12-22 18:08:35.793787: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:08:35.793976: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:08:35.794087: current val_eval_criterion_MA is 0.9794\n",
            "2019-12-22 18:08:35.794160: No improvement: current train MA -0.9605, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:08:35.794223: Patience: 11/50\n",
            "2019-12-22 18:08:35.794284: This epoch took 148.261749 s\n",
            "\n",
            "2019-12-22 18:08:35.794345: \n",
            "epoch:  37\n",
            "2019-12-22 18:10:57.103686: train loss : -0.9666\n",
            "2019-12-22 18:11:03.945402: val loss (train=False): -0.9718\n",
            "2019-12-22 18:11:03.945789: Val glob dc per class: [0.9819551228638577]\n",
            "2019-12-22 18:11:04.978775: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:11:04.978956: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:11:04.979064: current val_eval_criterion_MA is 0.9797\n",
            "2019-12-22 18:11:04.979165: No improvement: current train MA -0.9609, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:11:04.979260: Patience: 12/50\n",
            "2019-12-22 18:11:04.979361: This epoch took 148.151148 s\n",
            "\n",
            "2019-12-22 18:11:04.979477: \n",
            "epoch:  38\n",
            "2019-12-22 18:13:26.099757: train loss : -0.9450\n",
            "2019-12-22 18:13:33.010982: val loss (train=False): -0.9480\n",
            "2019-12-22 18:13:33.011510: Val glob dc per class: [0.9673783898028875]\n",
            "2019-12-22 18:13:34.041411: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:13:34.041595: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:13:34.041678: current val_eval_criterion_MA is 0.9785\n",
            "2019-12-22 18:13:34.041770: No improvement: current train MA -0.9598, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:13:34.041829: Patience: 13/50\n",
            "2019-12-22 18:13:34.041885: This epoch took 148.031668 s\n",
            "\n",
            "2019-12-22 18:13:34.041943: \n",
            "epoch:  39\n",
            "2019-12-22 18:15:55.614173: train loss : -0.9548\n",
            "2019-12-22 18:16:02.407538: val loss (train=False): -0.9684\n",
            "2019-12-22 18:16:02.408035: Val glob dc per class: [0.9796082137247711]\n",
            "2019-12-22 18:16:03.445759: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:16:03.445927: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:16:03.446004: current val_eval_criterion_MA is 0.9786\n",
            "2019-12-22 18:16:03.446100: No improvement: current train MA -0.9595, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:16:03.446174: Patience: 14/50\n",
            "2019-12-22 18:16:03.446264: This epoch took 148.365741 s\n",
            "\n",
            "2019-12-22 18:16:03.446344: \n",
            "epoch:  40\n",
            "2019-12-22 18:18:24.389395: train loss : -0.9649\n",
            "2019-12-22 18:18:31.139756: val loss (train=False): -0.9721\n",
            "2019-12-22 18:18:31.140280: Val glob dc per class: [0.982128753834351]\n",
            "2019-12-22 18:18:32.165332: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:18:32.165621: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:18:32.165734: current val_eval_criterion_MA is 0.9789\n",
            "2019-12-22 18:18:32.165831: No improvement: current train MA -0.9598, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:18:32.165951: Patience: 15/50\n",
            "2019-12-22 18:18:32.166070: This epoch took 147.693560 s\n",
            "\n",
            "2019-12-22 18:18:32.166176: \n",
            "epoch:  41\n",
            "2019-12-22 18:20:53.405582: train loss : -0.9662\n",
            "2019-12-22 18:21:00.283219: val loss (train=False): -0.9721\n",
            "2019-12-22 18:21:00.283657: Val glob dc per class: [0.9821679224125195]\n",
            "2019-12-22 18:21:01.394351: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:21:01.394539: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:21:01.394646: current val_eval_criterion_MA is 0.9792\n",
            "2019-12-22 18:21:01.394729: No improvement: current train MA -0.9603, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:21:01.394804: Patience: 16/50\n",
            "2019-12-22 18:21:01.394871: This epoch took 148.117126 s\n",
            "\n",
            "2019-12-22 18:21:01.394939: \n",
            "epoch:  42\n",
            "2019-12-22 18:23:23.210800: train loss : -0.9671\n",
            "2019-12-22 18:23:30.078278: val loss (train=False): -0.9747\n",
            "2019-12-22 18:23:30.078705: Val glob dc per class: [0.9835869986973607]\n",
            "2019-12-22 18:23:31.195909: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:23:31.196134: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:23:31.196233: current val_eval_criterion_MA is 0.9797\n",
            "2019-12-22 18:23:31.196308: No improvement: current train MA -0.9608, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:23:31.196388: Patience: 17/50\n",
            "2019-12-22 18:23:31.196459: This epoch took 148.683396 s\n",
            "\n",
            "2019-12-22 18:23:31.196522: \n",
            "epoch:  43\n",
            "2019-12-22 18:25:52.575710: train loss : -0.9678\n",
            "2019-12-22 18:25:59.717639: val loss (train=False): -0.9747\n",
            "2019-12-22 18:25:59.718217: Val glob dc per class: [0.9839067274204351]\n",
            "2019-12-22 18:26:00.779642: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:26:00.779855: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:26:00.779962: current val_eval_criterion_MA is 0.9801\n",
            "2019-12-22 18:26:00.780108: No improvement: current train MA -0.9613, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:26:00.780201: Patience: 18/50\n",
            "2019-12-22 18:26:00.780312: This epoch took 148.521258 s\n",
            "\n",
            "2019-12-22 18:26:00.780422: \n",
            "epoch:  44\n",
            "2019-12-22 18:28:22.360691: train loss : -0.9686\n",
            "2019-12-22 18:28:29.339522: val loss (train=False): -0.9760\n",
            "2019-12-22 18:28:29.340077: Val glob dc per class: [0.9847956894399634]\n",
            "2019-12-22 18:28:30.440156: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:28:30.440378: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:28:30.440479: current val_eval_criterion_MA is 0.9806\n",
            "2019-12-22 18:28:30.440580: No improvement: current train MA -0.9618, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:28:30.440669: Patience: 19/50\n",
            "2019-12-22 18:28:30.440764: This epoch took 148.559201 s\n",
            "\n",
            "2019-12-22 18:28:30.440867: \n",
            "epoch:  45\n",
            "2019-12-22 18:30:52.283677: train loss : -0.9692\n",
            "2019-12-22 18:30:59.216960: val loss (train=False): -0.9742\n",
            "2019-12-22 18:30:59.217443: Val glob dc per class: [0.9835521977272508]\n",
            "2019-12-22 18:31:00.319731: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:31:00.319961: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:31:00.320075: current val_eval_criterion_MA is 0.9809\n",
            "2019-12-22 18:31:00.320175: No improvement: current train MA -0.9623, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:31:00.320276: Patience: 20/50\n",
            "2019-12-22 18:31:00.320380: This epoch took 148.776226 s\n",
            "\n",
            "2019-12-22 18:31:00.320490: \n",
            "epoch:  46\n",
            "2019-12-22 18:33:22.542241: train loss : -0.9695\n",
            "2019-12-22 18:33:29.383276: val loss (train=False): -0.9755\n",
            "2019-12-22 18:33:29.383751: Val glob dc per class: [0.984293396147852]\n",
            "2019-12-22 18:33:30.484753: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:33:30.485034: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:33:30.485149: current val_eval_criterion_MA is 0.9812\n",
            "2019-12-22 18:33:30.485271: No improvement: current train MA -0.9628, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:33:30.485412: Patience: 21/50\n",
            "2019-12-22 18:33:30.485545: This epoch took 149.062896 s\n",
            "\n",
            "2019-12-22 18:33:30.485663: \n",
            "epoch:  47\n",
            "2019-12-22 18:35:52.845415: train loss : -0.9696\n",
            "2019-12-22 18:35:59.674450: val loss (train=False): -0.9768\n",
            "2019-12-22 18:35:59.674932: Val glob dc per class: [0.9851941245885656]\n",
            "2019-12-22 18:36:00.771201: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:36:00.771468: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:36:00.771590: current val_eval_criterion_MA is 0.9816\n",
            "2019-12-22 18:36:00.771710: No improvement: current train MA -0.9633, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:36:00.771824: Patience: 22/50\n",
            "2019-12-22 18:36:00.771929: This epoch took 149.188900 s\n",
            "\n",
            "2019-12-22 18:36:00.772061: \n",
            "epoch:  48\n",
            "2019-12-22 18:38:23.031269: train loss : -0.9698\n",
            "2019-12-22 18:38:29.841469: val loss (train=False): -0.9764\n",
            "2019-12-22 18:38:29.841973: Val glob dc per class: [0.9850104972702105]\n",
            "2019-12-22 18:38:31.016459: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:38:31.016651: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:38:31.016751: current val_eval_criterion_MA is 0.9820\n",
            "2019-12-22 18:38:31.016855: No improvement: current train MA -0.9637, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:38:31.016956: Patience: 23/50\n",
            "2019-12-22 18:38:31.017085: This epoch took 149.069510 s\n",
            "\n",
            "2019-12-22 18:38:31.017192: \n",
            "epoch:  49\n",
            "2019-12-22 18:40:52.618819: train loss : -0.9704\n",
            "2019-12-22 18:40:59.342082: val loss (train=False): -0.9767\n",
            "2019-12-22 18:40:59.342525: Val glob dc per class: [0.985125291199376]\n",
            "2019-12-22 18:41:00.358412: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:41:00.358619: saving scheduled checkpoint file...\n",
            "2019-12-22 18:41:00.375051: saving checkpoint...\n",
            "2019-12-22 18:41:00.778692: done, saving took 0.42 seconds\n",
            "2019-12-22 18:41:00.782423: done\n",
            "2019-12-22 18:41:00.782542: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:41:00.782618: current val_eval_criterion_MA is 0.9823\n",
            "2019-12-22 18:41:00.782712: No improvement: current train MA -0.9642, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:41:00.782792: Patience: 24/50\n",
            "2019-12-22 18:41:00.782860: This epoch took 148.324994 s\n",
            "\n",
            "2019-12-22 18:41:00.782934: \n",
            "epoch:  50\n",
            "2019-12-22 18:43:23.843714: train loss : -0.9678\n",
            "2019-12-22 18:43:30.671836: val loss (train=False): -0.9764\n",
            "2019-12-22 18:43:30.672345: Val glob dc per class: [0.9848508721522976]\n",
            "2019-12-22 18:43:31.683198: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:43:31.683372: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:43:31.683455: current val_eval_criterion_MA is 0.9825\n",
            "2019-12-22 18:43:31.683554: No improvement: current train MA -0.9645, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:43:31.683642: Patience: 25/50\n",
            "2019-12-22 18:43:31.683738: This epoch took 149.889005 s\n",
            "\n",
            "2019-12-22 18:43:31.683824: \n",
            "epoch:  51\n",
            "2019-12-22 18:45:54.534546: train loss : -0.9694\n",
            "2019-12-22 18:46:01.431248: val loss (train=False): -0.9765\n",
            "2019-12-22 18:46:01.431615: Val glob dc per class: [0.9851806377260787]\n",
            "2019-12-22 18:46:02.442063: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:46:02.442267: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:46:02.442355: current val_eval_criterion_MA is 0.9828\n",
            "2019-12-22 18:46:02.442478: No improvement: current train MA -0.9648, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:46:02.442563: Patience: 26/50\n",
            "2019-12-22 18:46:02.442708: This epoch took 149.747494 s\n",
            "\n",
            "2019-12-22 18:46:02.442843: \n",
            "epoch:  52\n",
            "2019-12-22 18:48:25.268698: train loss : -0.9702\n",
            "2019-12-22 18:48:32.185553: val loss (train=False): -0.9781\n",
            "2019-12-22 18:48:32.185996: Val glob dc per class: [0.9861362973267948]\n",
            "2019-12-22 18:48:33.204406: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:48:33.204574: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:48:33.204659: current val_eval_criterion_MA is 0.9831\n",
            "2019-12-22 18:48:33.204749: No improvement: current train MA -0.9652, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:48:33.204836: Patience: 27/50\n",
            "2019-12-22 18:48:33.204921: This epoch took 149.742842 s\n",
            "\n",
            "2019-12-22 18:48:33.205771: \n",
            "epoch:  53\n",
            "2019-12-22 18:50:54.912868: train loss : -0.9711\n",
            "2019-12-22 18:51:01.700035: val loss (train=False): -0.9783\n",
            "2019-12-22 18:51:01.700433: Val glob dc per class: [0.9863441824671756]\n",
            "2019-12-22 18:51:02.718328: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:51:02.718571: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:51:02.718660: current val_eval_criterion_MA is 0.9834\n",
            "2019-12-22 18:51:02.718768: No improvement: current train MA -0.9656, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:51:02.718911: Patience: 28/50\n",
            "2019-12-22 18:51:02.719002: This epoch took 148.494340 s\n",
            "\n",
            "2019-12-22 18:51:02.719126: \n",
            "epoch:  54\n",
            "2019-12-22 18:53:24.123944: train loss : -0.9719\n",
            "2019-12-22 18:53:30.910152: val loss (train=False): -0.9789\n",
            "2019-12-22 18:53:30.910546: Val glob dc per class: [0.9866396094055784]\n",
            "2019-12-22 18:53:31.941836: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:53:31.942053: current best_val_eval_criterion_MA is 0.98350\n",
            "2019-12-22 18:53:31.942161: current val_eval_criterion_MA is 0.9838\n",
            "2019-12-22 18:53:31.942259: saving best epoch checkpoint...\n",
            "2019-12-22 18:53:31.959358: saving checkpoint...\n",
            "2019-12-22 18:53:32.399036: done, saving took 0.46 seconds\n",
            "2019-12-22 18:53:32.404188: No improvement: current train MA -0.9660, best: -0.9656, eps is 0.0005\n",
            "2019-12-22 18:53:32.404322: Patience: 29/50\n",
            "2019-12-22 18:53:32.404425: This epoch took 148.191083 s\n",
            "\n",
            "2019-12-22 18:53:32.404525: \n",
            "epoch:  55\n",
            "2019-12-22 18:55:54.886243: train loss : -0.9717\n",
            "2019-12-22 18:56:01.679179: val loss (train=False): -0.9784\n",
            "2019-12-22 18:56:01.679688: Val glob dc per class: [0.986261460264043]\n",
            "2019-12-22 18:56:02.702289: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:56:02.702475: current best_val_eval_criterion_MA is 0.98380\n",
            "2019-12-22 18:56:02.702562: current val_eval_criterion_MA is 0.9840\n",
            "2019-12-22 18:56:02.702643: saving best epoch checkpoint...\n",
            "2019-12-22 18:56:02.719962: saving checkpoint...\n",
            "2019-12-22 18:56:03.149752: done, saving took 0.45 seconds\n",
            "2019-12-22 18:56:03.153602: New best epoch (train loss MA): -0.9664\n",
            "2019-12-22 18:56:03.153760: Patience: 0/50\n",
            "2019-12-22 18:56:03.153871: This epoch took 149.274770 s\n",
            "\n",
            "2019-12-22 18:56:03.153973: \n",
            "epoch:  56\n",
            "2019-12-22 18:58:24.786290: train loss : -0.9725\n",
            "2019-12-22 18:58:31.598680: val loss (train=False): -0.9786\n",
            "2019-12-22 18:58:31.599138: Val glob dc per class: [0.9865751732645177]\n",
            "2019-12-22 18:58:32.618907: lr is now (scheduler) 0.0003\n",
            "2019-12-22 18:58:32.619100: current best_val_eval_criterion_MA is 0.98400\n",
            "2019-12-22 18:58:32.619189: current val_eval_criterion_MA is 0.9843\n",
            "2019-12-22 18:58:32.619255: saving best epoch checkpoint...\n",
            "2019-12-22 18:58:32.636383: saving checkpoint...\n",
            "2019-12-22 18:58:33.108504: done, saving took 0.49 seconds\n",
            "2019-12-22 18:58:33.112959: No improvement: current train MA -0.9669, best: -0.9664, eps is 0.0005\n",
            "2019-12-22 18:58:33.113123: Patience: 1/50\n",
            "2019-12-22 18:58:33.113248: This epoch took 148.444753 s\n",
            "\n",
            "2019-12-22 18:58:33.113349: \n",
            "epoch:  57\n",
            "2019-12-22 19:00:54.896561: train loss : -0.9711\n",
            "2019-12-22 19:01:01.651865: val loss (train=False): -0.9781\n",
            "2019-12-22 19:01:01.652293: Val glob dc per class: [0.9861950061080836]\n",
            "2019-12-22 19:01:02.676107: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:01:02.676312: current best_val_eval_criterion_MA is 0.98430\n",
            "2019-12-22 19:01:02.676429: current val_eval_criterion_MA is 0.9845\n",
            "2019-12-22 19:01:02.676515: saving best epoch checkpoint...\n",
            "2019-12-22 19:01:02.699795: saving checkpoint...\n",
            "2019-12-22 19:01:03.189119: done, saving took 0.51 seconds\n",
            "2019-12-22 19:01:03.192737: New best epoch (train loss MA): -0.9672\n",
            "2019-12-22 19:01:03.192856: Patience: 0/50\n",
            "2019-12-22 19:01:03.192937: This epoch took 148.538601 s\n",
            "\n",
            "2019-12-22 19:01:03.193026: \n",
            "epoch:  58\n",
            "2019-12-22 19:03:24.705104: train loss : -0.9725\n",
            "2019-12-22 19:03:31.455070: val loss (train=False): -0.9783\n",
            "2019-12-22 19:03:31.455549: Val glob dc per class: [0.9863830574414214]\n",
            "2019-12-22 19:03:32.483537: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:03:32.483712: current best_val_eval_criterion_MA is 0.98450\n",
            "2019-12-22 19:03:32.483791: current val_eval_criterion_MA is 0.9847\n",
            "2019-12-22 19:03:32.483854: saving best epoch checkpoint...\n",
            "2019-12-22 19:03:32.507169: saving checkpoint...\n",
            "2019-12-22 19:03:33.070299: done, saving took 0.59 seconds\n",
            "2019-12-22 19:03:33.086918: No improvement: current train MA -0.9675, best: -0.9672, eps is 0.0005\n",
            "2019-12-22 19:03:33.087137: Patience: 1/50\n",
            "2019-12-22 19:03:33.087288: This epoch took 148.262157 s\n",
            "\n",
            "2019-12-22 19:03:33.087391: \n",
            "epoch:  59\n",
            "2019-12-22 19:05:54.557662: train loss : -0.9725\n",
            "2019-12-22 19:06:01.297737: val loss (train=False): -0.9798\n",
            "2019-12-22 19:06:01.298306: Val glob dc per class: [0.9872716598568801]\n",
            "2019-12-22 19:06:02.324439: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:06:02.324637: current best_val_eval_criterion_MA is 0.98470\n",
            "2019-12-22 19:06:02.324765: current val_eval_criterion_MA is 0.9849\n",
            "2019-12-22 19:06:02.324849: saving best epoch checkpoint...\n",
            "2019-12-22 19:06:02.341266: saving checkpoint...\n",
            "2019-12-22 19:06:02.840111: done, saving took 0.52 seconds\n",
            "2019-12-22 19:06:02.848956: New best epoch (train loss MA): -0.9679\n",
            "2019-12-22 19:06:02.849144: Patience: 0/50\n",
            "2019-12-22 19:06:02.849257: This epoch took 148.210495 s\n",
            "\n",
            "2019-12-22 19:06:02.849358: \n",
            "epoch:  60\n",
            "2019-12-22 19:08:24.198922: train loss : -0.9725\n",
            "2019-12-22 19:08:31.004964: val loss (train=False): -0.9780\n",
            "2019-12-22 19:08:31.005504: Val glob dc per class: [0.9858943915335698]\n",
            "2019-12-22 19:08:32.032267: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:08:32.032457: current best_val_eval_criterion_MA is 0.98490\n",
            "2019-12-22 19:08:32.032554: current val_eval_criterion_MA is 0.9850\n",
            "2019-12-22 19:08:32.032631: saving best epoch checkpoint...\n",
            "2019-12-22 19:08:32.048884: saving checkpoint...\n",
            "2019-12-22 19:08:32.538903: done, saving took 0.51 seconds\n",
            "2019-12-22 19:08:32.546955: No improvement: current train MA -0.9682, best: -0.9679, eps is 0.0005\n",
            "2019-12-22 19:08:32.547146: Patience: 1/50\n",
            "2019-12-22 19:08:32.547251: This epoch took 148.155725 s\n",
            "\n",
            "2019-12-22 19:08:32.547351: \n",
            "epoch:  61\n",
            "2019-12-22 19:10:55.269287: train loss : -0.9738\n",
            "2019-12-22 19:11:02.155553: val loss (train=False): -0.9784\n",
            "2019-12-22 19:11:02.156115: Val glob dc per class: [0.9863019468566826]\n",
            "2019-12-22 19:11:03.179631: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:11:03.179824: current best_val_eval_criterion_MA is 0.98500\n",
            "2019-12-22 19:11:03.179909: current val_eval_criterion_MA is 0.9851\n",
            "2019-12-22 19:11:03.179968: saving best epoch checkpoint...\n",
            "2019-12-22 19:11:03.197334: saving checkpoint...\n",
            "2019-12-22 19:11:03.719930: done, saving took 0.54 seconds\n",
            "2019-12-22 19:11:03.724236: New best epoch (train loss MA): -0.9686\n",
            "2019-12-22 19:11:03.724346: Patience: 0/50\n",
            "2019-12-22 19:11:03.724422: This epoch took 149.608336 s\n",
            "\n",
            "2019-12-22 19:11:03.724492: \n",
            "epoch:  62\n",
            "2019-12-22 19:13:26.224346: train loss : -0.9738\n",
            "2019-12-22 19:13:33.125598: val loss (train=False): -0.9803\n",
            "2019-12-22 19:13:33.126069: Val glob dc per class: [0.9876692585041343]\n",
            "2019-12-22 19:13:34.159466: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:13:34.159640: current best_val_eval_criterion_MA is 0.98510\n",
            "2019-12-22 19:13:34.159738: current val_eval_criterion_MA is 0.9854\n",
            "2019-12-22 19:13:34.159821: saving best epoch checkpoint...\n",
            "2019-12-22 19:13:34.175999: saving checkpoint...\n",
            "2019-12-22 19:13:34.677275: done, saving took 0.52 seconds\n",
            "2019-12-22 19:13:34.682178: No improvement: current train MA -0.9690, best: -0.9686, eps is 0.0005\n",
            "2019-12-22 19:13:34.682303: Patience: 1/50\n",
            "2019-12-22 19:13:34.682395: This epoch took 149.401222 s\n",
            "\n",
            "2019-12-22 19:13:34.682467: \n",
            "epoch:  63\n",
            "2019-12-22 19:15:57.068673: train loss : -0.9736\n",
            "2019-12-22 19:16:04.055460: val loss (train=False): -0.9807\n",
            "2019-12-22 19:16:04.055849: Val glob dc per class: [0.9878662487822603]\n",
            "2019-12-22 19:16:05.080906: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:16:05.081128: current best_val_eval_criterion_MA is 0.98540\n",
            "2019-12-22 19:16:05.081218: current val_eval_criterion_MA is 0.9856\n",
            "2019-12-22 19:16:05.081300: saving best epoch checkpoint...\n",
            "2019-12-22 19:16:05.097986: saving checkpoint...\n",
            "2019-12-22 19:16:05.578524: done, saving took 0.50 seconds\n",
            "2019-12-22 19:16:05.582519: New best epoch (train loss MA): -0.9693\n",
            "2019-12-22 19:16:05.582636: Patience: 0/50\n",
            "2019-12-22 19:16:05.582715: This epoch took 149.373086 s\n",
            "\n",
            "2019-12-22 19:16:05.582785: \n",
            "epoch:  64\n",
            "2019-12-22 19:18:28.755663: train loss : -0.9726\n",
            "2019-12-22 19:18:35.758596: val loss (train=False): -0.9803\n",
            "2019-12-22 19:18:35.759092: Val glob dc per class: [0.9875176327652105]\n",
            "2019-12-22 19:18:36.785900: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:18:36.786092: current best_val_eval_criterion_MA is 0.98560\n",
            "2019-12-22 19:18:36.786185: current val_eval_criterion_MA is 0.9858\n",
            "2019-12-22 19:18:36.786269: saving best epoch checkpoint...\n",
            "2019-12-22 19:18:36.802991: saving checkpoint...\n",
            "2019-12-22 19:18:37.315227: done, saving took 0.53 seconds\n",
            "2019-12-22 19:18:37.319134: No improvement: current train MA -0.9695, best: -0.9693, eps is 0.0005\n",
            "2019-12-22 19:18:37.319255: Patience: 1/50\n",
            "2019-12-22 19:18:37.319334: This epoch took 150.175931 s\n",
            "\n",
            "2019-12-22 19:18:37.319406: \n",
            "epoch:  65\n",
            "2019-12-22 19:20:59.691059: train loss : -0.9733\n",
            "2019-12-22 19:21:06.507004: val loss (train=False): -0.9803\n",
            "2019-12-22 19:21:06.507524: Val glob dc per class: [0.987484756340769]\n",
            "2019-12-22 19:21:07.528098: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:21:07.528277: current best_val_eval_criterion_MA is 0.98580\n",
            "2019-12-22 19:21:07.528366: current val_eval_criterion_MA is 0.9860\n",
            "2019-12-22 19:21:07.528428: saving best epoch checkpoint...\n",
            "2019-12-22 19:21:07.545113: saving checkpoint...\n",
            "2019-12-22 19:21:08.016744: done, saving took 0.49 seconds\n",
            "2019-12-22 19:21:08.022371: No improvement: current train MA -0.9698, best: -0.9693, eps is 0.0005\n",
            "2019-12-22 19:21:08.022509: Patience: 2/50\n",
            "2019-12-22 19:21:08.022611: This epoch took 149.187774 s\n",
            "\n",
            "2019-12-22 19:21:08.022718: \n",
            "epoch:  66\n",
            "2019-12-22 19:23:30.595269: train loss : -0.9735\n",
            "2019-12-22 19:23:37.450378: val loss (train=False): -0.9803\n",
            "2019-12-22 19:23:37.450836: Val glob dc per class: [0.987701969469469]\n",
            "2019-12-22 19:23:38.481796: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:23:38.481984: current best_val_eval_criterion_MA is 0.98600\n",
            "2019-12-22 19:23:38.482090: current val_eval_criterion_MA is 0.9862\n",
            "2019-12-22 19:23:38.482175: saving best epoch checkpoint...\n",
            "2019-12-22 19:23:38.499106: saving checkpoint...\n",
            "2019-12-22 19:23:38.969620: done, saving took 0.49 seconds\n",
            "2019-12-22 19:23:38.973471: New best epoch (train loss MA): -0.9700\n",
            "2019-12-22 19:23:38.973591: Patience: 0/50\n",
            "2019-12-22 19:23:38.973670: This epoch took 149.427773 s\n",
            "\n",
            "2019-12-22 19:23:38.973740: \n",
            "epoch:  67\n",
            "2019-12-22 19:26:01.394562: train loss : -0.9737\n",
            "2019-12-22 19:26:08.233299: val loss (train=False): -0.9808\n",
            "2019-12-22 19:26:08.233693: Val glob dc per class: [0.9879077637646155]\n",
            "2019-12-22 19:26:09.265867: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:26:09.266060: current best_val_eval_criterion_MA is 0.98620\n",
            "2019-12-22 19:26:09.266153: current val_eval_criterion_MA is 0.9863\n",
            "2019-12-22 19:26:09.266235: saving best epoch checkpoint...\n",
            "2019-12-22 19:26:09.282928: saving checkpoint...\n",
            "2019-12-22 19:26:09.793317: done, saving took 0.53 seconds\n",
            "2019-12-22 19:26:09.797743: No improvement: current train MA -0.9703, best: -0.9700, eps is 0.0005\n",
            "2019-12-22 19:26:09.797883: Patience: 1/50\n",
            "2019-12-22 19:26:09.797987: This epoch took 149.259656 s\n",
            "\n",
            "2019-12-22 19:26:09.798125: \n",
            "epoch:  68\n",
            "2019-12-22 19:28:31.722306: train loss : -0.9741\n",
            "2019-12-22 19:28:38.566183: val loss (train=False): -0.9806\n",
            "2019-12-22 19:28:38.566606: Val glob dc per class: [0.9878422737731335]\n",
            "2019-12-22 19:28:39.661199: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:28:39.661392: current best_val_eval_criterion_MA is 0.98630\n",
            "2019-12-22 19:28:39.661493: current val_eval_criterion_MA is 0.9865\n",
            "2019-12-22 19:28:39.661593: saving best epoch checkpoint...\n",
            "2019-12-22 19:28:39.678310: saving checkpoint...\n",
            "2019-12-22 19:28:40.157445: done, saving took 0.50 seconds\n",
            "2019-12-22 19:28:40.162521: New best epoch (train loss MA): -0.9706\n",
            "2019-12-22 19:28:40.162659: Patience: 0/50\n",
            "2019-12-22 19:28:40.162758: This epoch took 148.768125 s\n",
            "\n",
            "2019-12-22 19:28:40.162854: \n",
            "epoch:  69\n",
            "2019-12-22 19:31:02.920522: train loss : -0.9741\n",
            "2019-12-22 19:31:09.759773: val loss (train=False): -0.9804\n",
            "2019-12-22 19:31:09.760261: Val glob dc per class: [0.987755875610332]\n",
            "2019-12-22 19:31:10.843155: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:31:10.843382: current best_val_eval_criterion_MA is 0.98650\n",
            "2019-12-22 19:31:10.843475: current val_eval_criterion_MA is 0.9866\n",
            "2019-12-22 19:31:10.843541: saving best epoch checkpoint...\n",
            "2019-12-22 19:31:10.860960: saving checkpoint...\n",
            "2019-12-22 19:31:11.415956: done, saving took 0.57 seconds\n",
            "2019-12-22 19:31:11.430244: No improvement: current train MA -0.9708, best: -0.9706, eps is 0.0005\n",
            "2019-12-22 19:31:11.430381: Patience: 1/50\n",
            "2019-12-22 19:31:11.430458: This epoch took 149.597019 s\n",
            "\n",
            "2019-12-22 19:31:11.430527: \n",
            "epoch:  70\n",
            "2019-12-22 19:33:33.558877: train loss : -0.9742\n",
            "2019-12-22 19:33:40.398535: val loss (train=False): -0.9807\n",
            "2019-12-22 19:33:40.398983: Val glob dc per class: [0.9877444960950679]\n",
            "2019-12-22 19:33:41.468807: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:33:41.468984: current best_val_eval_criterion_MA is 0.98660\n",
            "2019-12-22 19:33:41.469083: current val_eval_criterion_MA is 0.9867\n",
            "2019-12-22 19:33:41.469149: saving best epoch checkpoint...\n",
            "2019-12-22 19:33:41.485774: saving checkpoint...\n",
            "2019-12-22 19:33:42.033834: done, saving took 0.56 seconds\n",
            "2019-12-22 19:33:42.040769: No improvement: current train MA -0.9710, best: -0.9706, eps is 0.0005\n",
            "2019-12-22 19:33:42.040946: Patience: 2/50\n",
            "2019-12-22 19:33:42.041085: This epoch took 148.968153 s\n",
            "\n",
            "2019-12-22 19:33:42.041187: \n",
            "epoch:  71\n",
            "2019-12-22 19:36:03.815698: train loss : -0.9745\n",
            "2019-12-22 19:36:10.598588: val loss (train=False): -0.9809\n",
            "2019-12-22 19:36:10.599125: Val glob dc per class: [0.9878748547976185]\n",
            "2019-12-22 19:36:11.677924: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:36:11.678158: current best_val_eval_criterion_MA is 0.98670\n",
            "2019-12-22 19:36:11.678257: current val_eval_criterion_MA is 0.9868\n",
            "2019-12-22 19:36:11.678341: saving best epoch checkpoint...\n",
            "2019-12-22 19:36:11.695071: saving checkpoint...\n",
            "2019-12-22 19:36:12.211429: done, saving took 0.53 seconds\n",
            "2019-12-22 19:36:12.218213: New best epoch (train loss MA): -0.9713\n",
            "2019-12-22 19:36:12.218396: Patience: 0/50\n",
            "2019-12-22 19:36:12.218513: This epoch took 148.557359 s\n",
            "\n",
            "2019-12-22 19:36:12.218621: \n",
            "epoch:  72\n",
            "2019-12-22 19:38:36.590578: train loss : -0.9736\n",
            "2019-12-22 19:38:43.831463: val loss (train=False): -0.9804\n",
            "2019-12-22 19:38:43.832035: Val glob dc per class: [0.9876656650425704]\n",
            "2019-12-22 19:38:44.913137: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:38:44.913320: current best_val_eval_criterion_MA is 0.98680\n",
            "2019-12-22 19:38:44.913412: current val_eval_criterion_MA is 0.9869\n",
            "2019-12-22 19:38:44.913498: saving best epoch checkpoint...\n",
            "2019-12-22 19:38:44.929482: saving checkpoint...\n",
            "2019-12-22 19:38:45.425193: done, saving took 0.51 seconds\n",
            "2019-12-22 19:38:45.429503: No improvement: current train MA -0.9714, best: -0.9713, eps is 0.0005\n",
            "2019-12-22 19:38:45.429618: Patience: 1/50\n",
            "2019-12-22 19:38:45.429693: This epoch took 151.612929 s\n",
            "\n",
            "2019-12-22 19:38:45.429772: \n",
            "epoch:  73\n",
            "2019-12-22 19:41:09.025519: train loss : -0.9746\n",
            "2019-12-22 19:41:15.961256: val loss (train=False): -0.9810\n",
            "2019-12-22 19:41:15.961685: Val glob dc per class: [0.9881263175895457]\n",
            "2019-12-22 19:41:17.042261: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:41:17.042446: current best_val_eval_criterion_MA is 0.98690\n",
            "2019-12-22 19:41:17.042523: current val_eval_criterion_MA is 0.9870\n",
            "2019-12-22 19:41:17.042589: saving best epoch checkpoint...\n",
            "2019-12-22 19:41:17.059450: saving checkpoint...\n",
            "2019-12-22 19:41:17.573127: done, saving took 0.53 seconds\n",
            "2019-12-22 19:41:17.579159: No improvement: current train MA -0.9717, best: -0.9713, eps is 0.0005\n",
            "2019-12-22 19:41:17.579288: Patience: 2/50\n",
            "2019-12-22 19:41:17.579370: This epoch took 150.531600 s\n",
            "\n",
            "2019-12-22 19:41:17.579440: \n",
            "epoch:  74\n",
            "2019-12-22 19:43:41.134735: train loss : -0.9746\n",
            "2019-12-22 19:43:48.042461: val loss (train=False): -0.9813\n",
            "2019-12-22 19:43:48.042922: Val glob dc per class: [0.9881560214781875]\n",
            "2019-12-22 19:43:49.107965: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:43:49.108187: current best_val_eval_criterion_MA is 0.98700\n",
            "2019-12-22 19:43:49.108276: current val_eval_criterion_MA is 0.9872\n",
            "2019-12-22 19:43:49.108355: saving best epoch checkpoint...\n",
            "2019-12-22 19:43:49.125379: saving checkpoint...\n",
            "2019-12-22 19:43:49.621252: done, saving took 0.51 seconds\n",
            "2019-12-22 19:43:49.627380: New best epoch (train loss MA): -0.9719\n",
            "2019-12-22 19:43:49.627527: Patience: 0/50\n",
            "2019-12-22 19:43:49.627650: This epoch took 150.463133 s\n",
            "\n",
            "2019-12-22 19:43:49.627765: \n",
            "epoch:  75\n",
            "2019-12-22 19:46:13.216749: train loss : -0.9745\n",
            "2019-12-22 19:46:20.227804: val loss (train=False): -0.9825\n",
            "2019-12-22 19:46:20.228298: Val glob dc per class: [0.989216344276591]\n",
            "2019-12-22 19:46:21.315834: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:46:21.316167: current best_val_eval_criterion_MA is 0.98720\n",
            "2019-12-22 19:46:21.316262: current val_eval_criterion_MA is 0.9874\n",
            "2019-12-22 19:46:21.316379: saving best epoch checkpoint...\n",
            "2019-12-22 19:46:21.333266: saving checkpoint...\n",
            "2019-12-22 19:46:21.832944: done, saving took 0.52 seconds\n",
            "2019-12-22 19:46:21.838616: No improvement: current train MA -0.9721, best: -0.9719, eps is 0.0005\n",
            "2019-12-22 19:46:21.838760: Patience: 1/50\n",
            "2019-12-22 19:46:21.838840: This epoch took 150.600149 s\n",
            "\n",
            "2019-12-22 19:46:21.838923: \n",
            "epoch:  76\n",
            "2019-12-22 19:48:44.260115: train loss : -0.9726\n",
            "2019-12-22 19:48:51.184185: val loss (train=False): -0.9703\n",
            "2019-12-22 19:48:51.184629: Val glob dc per class: [0.9809995562564314]\n",
            "2019-12-22 19:48:52.272976: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:48:52.273239: current best_val_eval_criterion_MA is 0.98740\n",
            "2019-12-22 19:48:52.273373: current val_eval_criterion_MA is 0.9867\n",
            "2019-12-22 19:48:52.273488: No improvement: current train MA -0.9721, best: -0.9719, eps is 0.0005\n",
            "2019-12-22 19:48:52.273580: Patience: 2/50\n",
            "2019-12-22 19:48:52.273718: This epoch took 149.345375 s\n",
            "\n",
            "2019-12-22 19:48:52.273805: \n",
            "epoch:  77\n",
            "2019-12-22 19:51:15.275602: train loss : -0.9654\n",
            "2019-12-22 19:51:22.276845: val loss (train=False): -0.9786\n",
            "2019-12-22 19:51:22.277401: Val glob dc per class: [0.9866925226910437]\n",
            "2019-12-22 19:51:23.371439: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:51:23.371675: current best_val_eval_criterion_MA is 0.98740\n",
            "2019-12-22 19:51:23.371784: current val_eval_criterion_MA is 0.9867\n",
            "2019-12-22 19:51:23.371875: No improvement: current train MA -0.9716, best: -0.9719, eps is 0.0005\n",
            "2019-12-22 19:51:23.371948: Patience: 3/50\n",
            "2019-12-22 19:51:23.372052: This epoch took 150.003230 s\n",
            "\n",
            "2019-12-22 19:51:23.372119: \n",
            "epoch:  78\n",
            "2019-12-22 19:53:45.658943: train loss : -0.9725\n",
            "2019-12-22 19:53:52.494555: val loss (train=False): -0.9772\n",
            "2019-12-22 19:53:52.494968: Val glob dc per class: [0.9856772741829906]\n",
            "2019-12-22 19:53:53.579671: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:53:53.579847: current best_val_eval_criterion_MA is 0.98740\n",
            "2019-12-22 19:53:53.579924: current val_eval_criterion_MA is 0.9866\n",
            "2019-12-22 19:53:53.580004: No improvement: current train MA -0.9717, best: -0.9719, eps is 0.0005\n",
            "2019-12-22 19:53:53.580089: Patience: 4/50\n",
            "2019-12-22 19:53:53.580151: This epoch took 149.122554 s\n",
            "\n",
            "2019-12-22 19:53:53.580212: \n",
            "epoch:  79\n",
            "2019-12-22 19:56:16.159692: train loss : -0.9735\n",
            "2019-12-22 19:56:23.120402: val loss (train=False): -0.9812\n",
            "2019-12-22 19:56:23.120860: Val glob dc per class: [0.988044537566446]\n",
            "2019-12-22 19:56:24.221116: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:56:24.221334: current best_val_eval_criterion_MA is 0.98740\n",
            "2019-12-22 19:56:24.221433: current val_eval_criterion_MA is 0.9868\n",
            "2019-12-22 19:56:24.221536: No improvement: current train MA -0.9718, best: -0.9719, eps is 0.0005\n",
            "2019-12-22 19:56:24.221635: Patience: 5/50\n",
            "2019-12-22 19:56:24.221730: This epoch took 149.540325 s\n",
            "\n",
            "2019-12-22 19:56:24.221827: \n",
            "epoch:  80\n",
            "2019-12-22 19:58:46.461106: train loss : -0.9746\n",
            "2019-12-22 19:58:53.288149: val loss (train=False): -0.9809\n",
            "2019-12-22 19:58:53.288644: Val glob dc per class: [0.9880926010721742]\n",
            "2019-12-22 19:58:54.379140: lr is now (scheduler) 0.0003\n",
            "2019-12-22 19:58:54.379345: current best_val_eval_criterion_MA is 0.98740\n",
            "2019-12-22 19:58:54.379462: current val_eval_criterion_MA is 0.9869\n",
            "2019-12-22 19:58:54.379564: No improvement: current train MA -0.9720, best: -0.9719, eps is 0.0005\n",
            "2019-12-22 19:58:54.379680: Patience: 6/50\n",
            "2019-12-22 19:58:54.379776: This epoch took 149.066466 s\n",
            "\n",
            "2019-12-22 19:58:54.379900: \n",
            "epoch:  81\n",
            "2019-12-22 20:01:16.145623: train loss : -0.9743\n",
            "2019-12-22 20:01:23.023577: val loss (train=False): -0.9823\n",
            "2019-12-22 20:01:23.024064: Val glob dc per class: [0.9889346817632547]\n",
            "2019-12-22 20:01:24.199816: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:01:24.200079: current best_val_eval_criterion_MA is 0.98740\n",
            "2019-12-22 20:01:24.200173: current val_eval_criterion_MA is 0.9871\n",
            "2019-12-22 20:01:24.200255: No improvement: current train MA -0.9722, best: -0.9719, eps is 0.0005\n",
            "2019-12-22 20:01:24.200320: Patience: 7/50\n",
            "2019-12-22 20:01:24.200382: This epoch took 148.643788 s\n",
            "\n",
            "2019-12-22 20:01:24.200460: \n",
            "epoch:  82\n",
            "2019-12-22 20:03:45.827603: train loss : -0.9756\n",
            "2019-12-22 20:03:52.704803: val loss (train=False): -0.9817\n",
            "2019-12-22 20:03:52.705252: Val glob dc per class: [0.9886419784621451]\n",
            "2019-12-22 20:03:53.788829: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:03:53.789055: current best_val_eval_criterion_MA is 0.98740\n",
            "2019-12-22 20:03:53.789213: current val_eval_criterion_MA is 0.9873\n",
            "2019-12-22 20:03:53.789311: New best epoch (train loss MA): -0.9724\n",
            "2019-12-22 20:03:53.789399: Patience: 0/50\n",
            "2019-12-22 20:03:53.789498: This epoch took 148.504467 s\n",
            "\n",
            "2019-12-22 20:03:53.789594: \n",
            "epoch:  83\n",
            "2019-12-22 20:06:17.597391: train loss : -0.9751\n",
            "2019-12-22 20:06:24.557329: val loss (train=False): -0.9827\n",
            "2019-12-22 20:06:24.557896: Val glob dc per class: [0.9892776591083637]\n",
            "2019-12-22 20:06:25.642740: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:06:25.642929: current best_val_eval_criterion_MA is 0.98740\n",
            "2019-12-22 20:06:25.643043: current val_eval_criterion_MA is 0.9875\n",
            "2019-12-22 20:06:25.643136: saving best epoch checkpoint...\n",
            "2019-12-22 20:06:25.659683: saving checkpoint...\n",
            "2019-12-22 20:06:26.242915: done, saving took 0.60 seconds\n",
            "2019-12-22 20:06:26.252153: No improvement: current train MA -0.9726, best: -0.9724, eps is 0.0005\n",
            "2019-12-22 20:06:26.255059: Patience: 1/50\n",
            "2019-12-22 20:06:26.255165: This epoch took 150.767744 s\n",
            "\n",
            "2019-12-22 20:06:26.255252: \n",
            "epoch:  84\n",
            "2019-12-22 20:08:49.870458: train loss : -0.9751\n",
            "2019-12-22 20:08:56.810862: val loss (train=False): -0.9826\n",
            "2019-12-22 20:08:56.811383: Val glob dc per class: [0.9891155603015055]\n",
            "2019-12-22 20:08:57.895465: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:08:57.895671: current best_val_eval_criterion_MA is 0.98750\n",
            "2019-12-22 20:08:57.895776: current val_eval_criterion_MA is 0.9876\n",
            "2019-12-22 20:08:57.895873: saving best epoch checkpoint...\n",
            "2019-12-22 20:08:57.912753: saving checkpoint...\n",
            "2019-12-22 20:08:58.472487: done, saving took 0.58 seconds\n",
            "2019-12-22 20:08:58.477986: No improvement: current train MA -0.9728, best: -0.9724, eps is 0.0005\n",
            "2019-12-22 20:08:58.478140: Patience: 2/50\n",
            "2019-12-22 20:08:58.478247: This epoch took 150.555769 s\n",
            "\n",
            "2019-12-22 20:08:58.478344: \n",
            "epoch:  85\n",
            "2019-12-22 20:11:21.982626: train loss : -0.9754\n",
            "2019-12-22 20:11:28.838217: val loss (train=False): -0.9827\n",
            "2019-12-22 20:11:28.838660: Val glob dc per class: [0.9893382303735685]\n",
            "2019-12-22 20:11:29.929916: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:11:29.930129: current best_val_eval_criterion_MA is 0.98760\n",
            "2019-12-22 20:11:29.930245: current val_eval_criterion_MA is 0.9878\n",
            "2019-12-22 20:11:29.930382: saving best epoch checkpoint...\n",
            "2019-12-22 20:11:29.947140: saving checkpoint...\n",
            "2019-12-22 20:11:30.520508: done, saving took 0.59 seconds\n",
            "2019-12-22 20:11:30.527039: New best epoch (train loss MA): -0.9729\n",
            "2019-12-22 20:11:30.527186: Patience: 0/50\n",
            "2019-12-22 20:11:30.527563: This epoch took 150.359973 s\n",
            "\n",
            "2019-12-22 20:11:30.527717: \n",
            "epoch:  86\n",
            "2019-12-22 20:13:53.391834: train loss : -0.9757\n",
            "2019-12-22 20:14:00.257582: val loss (train=False): -0.9831\n",
            "2019-12-22 20:14:00.258066: Val glob dc per class: [0.9893429319900602]\n",
            "2019-12-22 20:14:01.338187: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:14:01.338403: current best_val_eval_criterion_MA is 0.98780\n",
            "2019-12-22 20:14:01.338502: current val_eval_criterion_MA is 0.9879\n",
            "2019-12-22 20:14:01.338595: saving best epoch checkpoint...\n",
            "2019-12-22 20:14:01.356707: saving checkpoint...\n",
            "2019-12-22 20:14:01.880440: done, saving took 0.54 seconds\n",
            "2019-12-22 20:14:01.887861: No improvement: current train MA -0.9731, best: -0.9729, eps is 0.0005\n",
            "2019-12-22 20:14:01.887995: Patience: 1/50\n",
            "2019-12-22 20:14:01.888134: This epoch took 149.729934 s\n",
            "\n",
            "2019-12-22 20:14:01.888236: \n",
            "epoch:  87\n",
            "2019-12-22 20:16:24.858549: train loss : -0.9755\n",
            "2019-12-22 20:16:31.749773: val loss (train=False): -0.9830\n",
            "2019-12-22 20:16:31.750294: Val glob dc per class: [0.9893536864627996]\n",
            "2019-12-22 20:16:32.802867: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:16:32.803054: current best_val_eval_criterion_MA is 0.98790\n",
            "2019-12-22 20:16:32.803192: current val_eval_criterion_MA is 0.9881\n",
            "2019-12-22 20:16:32.803276: saving best epoch checkpoint...\n",
            "2019-12-22 20:16:32.821100: saving checkpoint...\n",
            "2019-12-22 20:16:33.324842: done, saving took 0.52 seconds\n",
            "2019-12-22 20:16:33.332560: No improvement: current train MA -0.9733, best: -0.9729, eps is 0.0005\n",
            "2019-12-22 20:16:33.332718: Patience: 2/50\n",
            "2019-12-22 20:16:33.332829: This epoch took 149.861653 s\n",
            "\n",
            "2019-12-22 20:16:33.332941: \n",
            "epoch:  88\n",
            "2019-12-22 20:18:56.289454: train loss : -0.9755\n",
            "2019-12-22 20:19:03.112249: val loss (train=False): -0.9828\n",
            "2019-12-22 20:19:03.112695: Val glob dc per class: [0.9892176714434917]\n",
            "2019-12-22 20:19:04.158422: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:19:04.158596: current best_val_eval_criterion_MA is 0.98810\n",
            "2019-12-22 20:19:04.158673: current val_eval_criterion_MA is 0.9882\n",
            "2019-12-22 20:19:04.158736: saving best epoch checkpoint...\n",
            "2019-12-22 20:19:04.175440: saving checkpoint...\n",
            "2019-12-22 20:19:04.693652: done, saving took 0.53 seconds\n",
            "2019-12-22 20:19:04.700063: New best epoch (train loss MA): -0.9735\n",
            "2019-12-22 20:19:04.700217: Patience: 0/50\n",
            "2019-12-22 20:19:04.700323: This epoch took 149.779347 s\n",
            "\n",
            "2019-12-22 20:19:04.700420: \n",
            "epoch:  89\n",
            "2019-12-22 20:21:27.752191: train loss : -0.9754\n",
            "2019-12-22 20:21:34.634802: val loss (train=False): -0.9836\n",
            "2019-12-22 20:21:34.635239: Val glob dc per class: [0.9896793458016964]\n",
            "2019-12-22 20:21:35.704704: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:21:35.704899: current best_val_eval_criterion_MA is 0.98820\n",
            "2019-12-22 20:21:35.705001: current val_eval_criterion_MA is 0.9883\n",
            "2019-12-22 20:21:35.705157: saving best epoch checkpoint...\n",
            "2019-12-22 20:21:35.721938: saving checkpoint...\n",
            "2019-12-22 20:21:36.256455: done, saving took 0.55 seconds\n",
            "2019-12-22 20:21:36.261584: No improvement: current train MA -0.9736, best: -0.9735, eps is 0.0005\n",
            "2019-12-22 20:21:36.261735: Patience: 1/50\n",
            "2019-12-22 20:21:36.261858: This epoch took 149.934458 s\n",
            "\n",
            "2019-12-22 20:21:36.261961: \n",
            "epoch:  90\n",
            "2019-12-22 20:23:58.443731: train loss : -0.9762\n",
            "2019-12-22 20:24:05.329833: val loss (train=False): -0.9843\n",
            "2019-12-22 20:24:05.330285: Val glob dc per class: [0.9902554172637908]\n",
            "2019-12-22 20:24:06.384980: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:24:06.385191: current best_val_eval_criterion_MA is 0.98830\n",
            "2019-12-22 20:24:06.385268: current val_eval_criterion_MA is 0.9885\n",
            "2019-12-22 20:24:06.385332: saving best epoch checkpoint...\n",
            "2019-12-22 20:24:06.402430: saving checkpoint...\n",
            "2019-12-22 20:24:06.893030: done, saving took 0.51 seconds\n",
            "2019-12-22 20:24:06.897562: No improvement: current train MA -0.9738, best: -0.9735, eps is 0.0005\n",
            "2019-12-22 20:24:06.897683: Patience: 2/50\n",
            "2019-12-22 20:24:06.897776: This epoch took 149.067940 s\n",
            "\n",
            "2019-12-22 20:24:06.897860: \n",
            "epoch:  91\n",
            "2019-12-22 20:26:29.059613: train loss : -0.9761\n",
            "2019-12-22 20:26:35.967940: val loss (train=False): -0.9837\n",
            "2019-12-22 20:26:35.968425: Val glob dc per class: [0.9899419782943539]\n",
            "2019-12-22 20:26:37.069805: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:26:37.070005: current best_val_eval_criterion_MA is 0.98850\n",
            "2019-12-22 20:26:37.070145: current val_eval_criterion_MA is 0.9887\n",
            "2019-12-22 20:26:37.070262: saving best epoch checkpoint...\n",
            "2019-12-22 20:26:37.087141: saving checkpoint...\n",
            "2019-12-22 20:26:37.631694: done, saving took 0.56 seconds\n",
            "2019-12-22 20:26:37.653152: No improvement: current train MA -0.9739, best: -0.9735, eps is 0.0005\n",
            "2019-12-22 20:26:37.653269: Patience: 3/50\n",
            "2019-12-22 20:26:37.653343: This epoch took 149.070215 s\n",
            "\n",
            "2019-12-22 20:26:37.653411: \n",
            "epoch:  92\n",
            "2019-12-22 20:28:59.918703: train loss : -0.9758\n",
            "2019-12-22 20:29:06.754491: val loss (train=False): -0.9839\n",
            "2019-12-22 20:29:06.755122: Val glob dc per class: [0.9901016768480178]\n",
            "2019-12-22 20:29:07.804315: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:29:07.804503: current best_val_eval_criterion_MA is 0.98870\n",
            "2019-12-22 20:29:07.804598: current val_eval_criterion_MA is 0.9888\n",
            "2019-12-22 20:29:07.804686: saving best epoch checkpoint...\n",
            "2019-12-22 20:29:07.820992: saving checkpoint...\n",
            "2019-12-22 20:29:08.336487: done, saving took 0.53 seconds\n",
            "2019-12-22 20:29:08.341672: New best epoch (train loss MA): -0.9741\n",
            "2019-12-22 20:29:08.341800: Patience: 0/50\n",
            "2019-12-22 20:29:08.341880: This epoch took 149.101290 s\n",
            "\n",
            "2019-12-22 20:29:08.341958: \n",
            "epoch:  93\n",
            "2019-12-22 20:31:29.735757: train loss : -0.9761\n",
            "2019-12-22 20:31:36.612410: val loss (train=False): -0.9837\n",
            "2019-12-22 20:31:36.612872: Val glob dc per class: [0.9899336476695321]\n",
            "2019-12-22 20:31:37.668687: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:31:37.668881: current best_val_eval_criterion_MA is 0.98880\n",
            "2019-12-22 20:31:37.668965: current val_eval_criterion_MA is 0.9889\n",
            "2019-12-22 20:31:37.669079: saving best epoch checkpoint...\n",
            "2019-12-22 20:31:37.686126: saving checkpoint...\n",
            "2019-12-22 20:31:38.226295: done, saving took 0.56 seconds\n",
            "2019-12-22 20:31:38.237499: No improvement: current train MA -0.9742, best: -0.9741, eps is 0.0005\n",
            "2019-12-22 20:31:38.237639: Patience: 1/50\n",
            "2019-12-22 20:31:38.237718: This epoch took 148.270566 s\n",
            "\n",
            "2019-12-22 20:31:38.237788: \n",
            "epoch:  94\n",
            "2019-12-22 20:34:01.308182: train loss : -0.9768\n",
            "2019-12-22 20:34:08.213530: val loss (train=False): -0.9836\n",
            "2019-12-22 20:34:08.214145: Val glob dc per class: [0.9898347452754866]\n",
            "2019-12-22 20:34:09.266471: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:34:09.266655: current best_val_eval_criterion_MA is 0.98890\n",
            "2019-12-22 20:34:09.266735: current val_eval_criterion_MA is 0.9890\n",
            "2019-12-22 20:34:09.266802: saving best epoch checkpoint...\n",
            "2019-12-22 20:34:09.283604: saving checkpoint...\n",
            "2019-12-22 20:34:09.788289: done, saving took 0.52 seconds\n",
            "2019-12-22 20:34:09.793835: No improvement: current train MA -0.9744, best: -0.9741, eps is 0.0005\n",
            "2019-12-22 20:34:09.793947: Patience: 2/50\n",
            "2019-12-22 20:34:09.794087: This epoch took 149.975939 s\n",
            "\n",
            "2019-12-22 20:34:09.794183: \n",
            "epoch:  95\n",
            "2019-12-22 20:36:32.907263: train loss : -0.9774\n",
            "2019-12-22 20:36:39.822422: val loss (train=False): -0.9836\n",
            "2019-12-22 20:36:39.823084: Val glob dc per class: [0.9896826315461442]\n",
            "2019-12-22 20:36:40.922839: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:36:40.923045: current best_val_eval_criterion_MA is 0.98900\n",
            "2019-12-22 20:36:40.923135: current val_eval_criterion_MA is 0.9891\n",
            "2019-12-22 20:36:40.923207: saving best epoch checkpoint...\n",
            "2019-12-22 20:36:40.939329: saving checkpoint...\n",
            "2019-12-22 20:36:41.495279: done, saving took 0.57 seconds\n",
            "2019-12-22 20:36:41.500157: New best epoch (train loss MA): -0.9746\n",
            "2019-12-22 20:36:41.500278: Patience: 0/50\n",
            "2019-12-22 20:36:41.500357: This epoch took 150.028404 s\n",
            "\n",
            "2019-12-22 20:36:41.500449: \n",
            "epoch:  96\n",
            "2019-12-22 20:39:04.599566: train loss : -0.9770\n",
            "2019-12-22 20:39:11.665388: val loss (train=False): -0.9833\n",
            "2019-12-22 20:39:11.665868: Val glob dc per class: [0.9897061655322831]\n",
            "2019-12-22 20:39:12.733826: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:39:12.734031: current best_val_eval_criterion_MA is 0.98910\n",
            "2019-12-22 20:39:12.734130: current val_eval_criterion_MA is 0.9892\n",
            "2019-12-22 20:39:12.734224: saving best epoch checkpoint...\n",
            "2019-12-22 20:39:12.752169: saving checkpoint...\n",
            "2019-12-22 20:39:13.356340: done, saving took 0.62 seconds\n",
            "2019-12-22 20:39:13.366360: No improvement: current train MA -0.9748, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 20:39:13.366814: Patience: 1/50\n",
            "2019-12-22 20:39:13.366962: This epoch took 150.165048 s\n",
            "\n",
            "2019-12-22 20:39:13.368122: \n",
            "epoch:  97\n",
            "2019-12-22 20:41:37.035426: train loss : -0.9755\n",
            "2019-12-22 20:41:43.960629: val loss (train=False): -0.9848\n",
            "2019-12-22 20:41:43.961134: Val glob dc per class: [0.9906171406461598]\n",
            "2019-12-22 20:41:45.008751: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:41:45.008937: current best_val_eval_criterion_MA is 0.98920\n",
            "2019-12-22 20:41:45.009074: current val_eval_criterion_MA is 0.9893\n",
            "2019-12-22 20:41:45.009179: saving best epoch checkpoint...\n",
            "2019-12-22 20:41:45.027079: saving checkpoint...\n",
            "2019-12-22 20:41:45.516724: done, saving took 0.51 seconds\n",
            "2019-12-22 20:41:45.522839: No improvement: current train MA -0.9748, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 20:41:45.522986: Patience: 2/50\n",
            "2019-12-22 20:41:45.523118: This epoch took 150.592561 s\n",
            "\n",
            "2019-12-22 20:41:45.523224: \n",
            "epoch:  98\n",
            "2019-12-22 20:44:08.720627: train loss : -0.9764\n",
            "2019-12-22 20:44:15.624121: val loss (train=False): -0.9844\n",
            "2019-12-22 20:44:15.624536: Val glob dc per class: [0.9903490289388802]\n",
            "2019-12-22 20:44:16.675174: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:44:16.675364: current best_val_eval_criterion_MA is 0.98930\n",
            "2019-12-22 20:44:16.675444: current val_eval_criterion_MA is 0.9894\n",
            "2019-12-22 20:44:16.675518: saving best epoch checkpoint...\n",
            "2019-12-22 20:44:16.692180: saving checkpoint...\n",
            "2019-12-22 20:44:17.199579: done, saving took 0.52 seconds\n",
            "2019-12-22 20:44:17.203711: No improvement: current train MA -0.9749, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 20:44:17.203833: Patience: 3/50\n",
            "2019-12-22 20:44:17.203912: This epoch took 150.100991 s\n",
            "\n",
            "2019-12-22 20:44:17.203981: \n",
            "epoch:  99\n",
            "2019-12-22 20:46:39.787677: train loss : -0.9715\n",
            "2019-12-22 20:46:46.655104: val loss (train=False): -0.9784\n",
            "2019-12-22 20:46:46.655533: Val glob dc per class: [0.9860448991269773]\n",
            "2019-12-22 20:46:47.705319: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:46:47.705485: saving scheduled checkpoint file...\n",
            "2019-12-22 20:46:47.722062: saving checkpoint...\n",
            "2019-12-22 20:46:48.206851: done, saving took 0.50 seconds\n",
            "2019-12-22 20:46:48.212748: done\n",
            "2019-12-22 20:46:48.212931: current best_val_eval_criterion_MA is 0.98940\n",
            "2019-12-22 20:46:48.213130: current val_eval_criterion_MA is 0.9891\n",
            "2019-12-22 20:46:48.213289: No improvement: current train MA -0.9747, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 20:46:48.213398: Patience: 4/50\n",
            "2019-12-22 20:46:48.213493: This epoch took 149.451208 s\n",
            "\n",
            "2019-12-22 20:46:48.213588: \n",
            "epoch:  100\n",
            "2019-12-22 20:49:10.877571: train loss : -0.9477\n",
            "2019-12-22 20:49:17.738219: val loss (train=False): -0.9721\n",
            "2019-12-22 20:49:17.738623: Val glob dc per class: [0.9820546653481379]\n",
            "2019-12-22 20:49:18.781786: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:49:18.781967: current best_val_eval_criterion_MA is 0.98940\n",
            "2019-12-22 20:49:18.782061: current val_eval_criterion_MA is 0.9884\n",
            "2019-12-22 20:49:18.782129: No improvement: current train MA -0.9728, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 20:49:18.782268: Patience: 5/50\n",
            "2019-12-22 20:49:18.782328: This epoch took 149.524714 s\n",
            "\n",
            "2019-12-22 20:49:18.782389: \n",
            "epoch:  101\n",
            "2019-12-22 20:51:40.845793: train loss : -0.9658\n",
            "2019-12-22 20:51:47.817379: val loss (train=False): -0.9771\n",
            "2019-12-22 20:51:47.817871: Val glob dc per class: [0.9853467273343252]\n",
            "2019-12-22 20:51:48.884333: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:51:48.884566: current best_val_eval_criterion_MA is 0.98940\n",
            "2019-12-22 20:51:48.884663: current val_eval_criterion_MA is 0.9881\n",
            "2019-12-22 20:51:48.884758: No improvement: current train MA -0.9723, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 20:51:48.884861: Patience: 6/50\n",
            "2019-12-22 20:51:48.884956: This epoch took 149.035168 s\n",
            "\n",
            "2019-12-22 20:51:48.885089: \n",
            "epoch:  102\n",
            "2019-12-22 20:54:10.772681: train loss : -0.9718\n",
            "2019-12-22 20:54:17.699826: val loss (train=False): -0.9783\n",
            "2019-12-22 20:54:17.700259: Val glob dc per class: [0.9862343461560723]\n",
            "2019-12-22 20:54:18.753574: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:54:18.753775: current best_val_eval_criterion_MA is 0.98940\n",
            "2019-12-22 20:54:18.753876: current val_eval_criterion_MA is 0.9879\n",
            "2019-12-22 20:54:18.753998: No improvement: current train MA -0.9723, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 20:54:18.754109: Patience: 7/50\n",
            "2019-12-22 20:54:18.754233: This epoch took 148.814801 s\n",
            "\n",
            "2019-12-22 20:54:18.754338: \n",
            "epoch:  103\n",
            "2019-12-22 20:56:40.350704: train loss : -0.9734\n",
            "2019-12-22 20:56:47.245437: val loss (train=False): -0.9806\n",
            "2019-12-22 20:56:47.246167: Val glob dc per class: [0.9879052093327537]\n",
            "2019-12-22 20:56:48.370708: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:56:48.370883: current best_val_eval_criterion_MA is 0.98940\n",
            "2019-12-22 20:56:48.370973: current val_eval_criterion_MA is 0.9879\n",
            "2019-12-22 20:56:48.371088: No improvement: current train MA -0.9723, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 20:56:48.371215: Patience: 8/50\n",
            "2019-12-22 20:56:48.371309: This epoch took 148.491202 s\n",
            "\n",
            "2019-12-22 20:56:48.371403: \n",
            "epoch:  104\n",
            "2019-12-22 20:59:10.309941: train loss : -0.9746\n",
            "2019-12-22 20:59:17.133896: val loss (train=False): -0.9815\n",
            "2019-12-22 20:59:17.134586: Val glob dc per class: [0.9883013234216644]\n",
            "2019-12-22 20:59:18.206602: lr is now (scheduler) 0.0003\n",
            "2019-12-22 20:59:18.206767: current best_val_eval_criterion_MA is 0.98940\n",
            "2019-12-22 20:59:18.206857: current val_eval_criterion_MA is 0.9879\n",
            "2019-12-22 20:59:18.206941: No improvement: current train MA -0.9725, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 20:59:18.207091: Patience: 9/50\n",
            "2019-12-22 20:59:18.207198: This epoch took 148.762682 s\n",
            "\n",
            "2019-12-22 20:59:18.207262: \n",
            "epoch:  105\n",
            "2019-12-22 21:01:42.209188: train loss : -0.9750\n",
            "2019-12-22 21:01:49.101445: val loss (train=False): -0.9827\n",
            "2019-12-22 21:01:49.101887: Val glob dc per class: [0.9892312175345754]\n",
            "2019-12-22 21:01:50.179380: lr is now (scheduler) 0.0003\n",
            "2019-12-22 21:01:50.179602: current best_val_eval_criterion_MA is 0.98940\n",
            "2019-12-22 21:01:50.179699: current val_eval_criterion_MA is 0.9881\n",
            "2019-12-22 21:01:50.179798: No improvement: current train MA -0.9727, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 21:01:50.179890: Patience: 10/50\n",
            "2019-12-22 21:01:50.179994: This epoch took 150.894341 s\n",
            "\n",
            "2019-12-22 21:01:50.180113: \n",
            "epoch:  106\n",
            "2019-12-22 21:04:13.607746: train loss : -0.9755\n",
            "2019-12-22 21:04:20.634323: val loss (train=False): -0.9830\n",
            "2019-12-22 21:04:20.634752: Val glob dc per class: [0.9893286895647035]\n",
            "2019-12-22 21:04:21.702980: lr is now (scheduler) 0.0003\n",
            "2019-12-22 21:04:21.703177: current best_val_eval_criterion_MA is 0.98940\n",
            "2019-12-22 21:04:21.703283: current val_eval_criterion_MA is 0.9882\n",
            "2019-12-22 21:04:21.703375: No improvement: current train MA -0.9729, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 21:04:21.703460: Patience: 11/50\n",
            "2019-12-22 21:04:21.703551: This epoch took 150.454287 s\n",
            "\n",
            "2019-12-22 21:04:21.703659: \n",
            "epoch:  107\n",
            "2019-12-22 21:06:44.408562: train loss : -0.9762\n",
            "2019-12-22 21:06:51.257653: val loss (train=False): -0.9841\n",
            "2019-12-22 21:06:51.258225: Val glob dc per class: [0.9901213287577169]\n",
            "2019-12-22 21:06:52.333870: lr is now (scheduler) 0.0003\n",
            "2019-12-22 21:06:52.334087: current best_val_eval_criterion_MA is 0.98940\n",
            "2019-12-22 21:06:52.334170: current val_eval_criterion_MA is 0.9884\n",
            "2019-12-22 21:06:52.334255: No improvement: current train MA -0.9731, best: -0.9746, eps is 0.0005\n",
            "2019-12-22 21:06:52.334364: Patience: 12/50\n",
            "2019-12-22 21:06:52.334426: This epoch took 149.554082 s\n",
            "\n",
            "2019-12-22 21:06:52.334504: \n",
            "epoch:  108\n",
            "Process Process-21:\n",
            "Process Process-10:\n",
            "Process Process-16:\n",
            "Process Process-15:\n",
            "Process Process-11:\n",
            "Process Process-14:\n",
            "Process Process-12:\n",
            "Process Process-25:\n",
            "Process Process-22:\n",
            "Process Process-26:\n",
            "Process Process-17:\n",
            "Process Process-23:\n",
            "Process Process-13:\n",
            "Process Process-24:\n",
            "Traceback (most recent call last):\n",
            "  File \"run/run_training.py\", line 103, in <module>\n",
            "    trainer.run_training()\n",
            "  File \"/content/nnUNet/nnunet/training/network_training/nnUNetTrainer.py\", line 275, in run_training\n",
            "    super(nnUNetTrainer, self).run_training()\n",
            "  File \"/content/nnUNet/nnunet/training/network_training/network_trainer.py\", line 351, in run_training\n",
            "    l = self.run_iteration(self.tr_gen, True)\n",
            "  File \"/content/nnUNet/nnunet/training/network_training/network_trainer.py\", line 534, in run_iteration\n",
            "    l = self.loss(output, target)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
            "Process Process-9:\n",
            "Process Process-18:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "    result = self.forward(*input, **kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/content/nnUNet/nnunet/training/loss_functions/dice_loss.py\", line 122, in forward\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "    dc_loss = self.dc(net_output, target)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 55, in producer\n",
            "    queue.put(item, timeout=2)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
            "    if not self._sem.acquire(block, timeout):\n",
            "Traceback (most recent call last):\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 541, in __call__\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "KeyboardInterrupt\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/nnUNet/nnunet/training/loss_functions/dice_loss.py\", line 100, in forward\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "    tp, fp, fn = get_tp_fp_fn(x, y, axes, loss_mask, self.square)\n",
            "  File \"/content/nnUNet/nnunet/training/loss_functions/dice_loss.py\", line 52, in get_tp_fp_fn\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "    y_onehot.scatter_(1, gt, 1)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/batchgenerators/dataloading/multi_threaded_augmenter.py\", line 67, in producer\n",
            "    raise KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UGqa6I-rLqm",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNDhwjmSsrCe",
        "colab_type": "code",
        "outputId": "95173957-9ff1-4bea-dba9-ad74e26a92b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!OMP_NUM_THREADS=1 python inference/predict_simple.py -h"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, Fabian, et al. \"nnU-Net: Breaking the Spell on Successful Medical Image Segmentation.\" arXiv preprint arXiv:1904.08128 (2019).\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "usage: predict_simple.py [-h] -i INPUT_FOLDER -o OUTPUT_FOLDER -t TASK_NAME\n",
            "                         [-tr NNUNET_TRAINER] [-m MODEL] [-p PLANS_IDENTIFIER]\n",
            "                         [-f FOLDS [FOLDS ...]] [-z] [-l LOWRES_SEGMENTATIONS]\n",
            "                         [--part_id PART_ID] [--num_parts NUM_PARTS]\n",
            "                         [--num_threads_preprocessing NUM_THREADS_PREPROCESSING]\n",
            "                         [--num_threads_nifti_save NUM_THREADS_NIFTI_SAVE]\n",
            "                         [--tta TTA] [--overwrite_existing OVERWRITE_EXISTING]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -i INPUT_FOLDER, --input_folder INPUT_FOLDER\n",
            "                        Must contain all modalities for each patient in the\n",
            "                        correct order (same as training). Files must be named\n",
            "                        CASENAME_XXXX.nii.gz where XXXX is the modality\n",
            "                        identifier (0000, 0001, etc)\n",
            "  -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER\n",
            "                        folder for saving predictions\n",
            "  -t TASK_NAME, --task_name TASK_NAME\n",
            "                        task name, required.\n",
            "  -tr NNUNET_TRAINER, --nnunet_trainer NNUNET_TRAINER\n",
            "                        nnUNet trainer class. Default: nnUNetTrainer\n",
            "  -m MODEL, --model MODEL\n",
            "                        2d, 3d_lowres, 3d_fullres or 3d_cascade_fullres.\n",
            "                        Default: 3d_fullres\n",
            "  -p PLANS_IDENTIFIER, --plans_identifier PLANS_IDENTIFIER\n",
            "                        do not touch this unless you know what you are doing\n",
            "  -f FOLDS [FOLDS ...], --folds FOLDS [FOLDS ...]\n",
            "                        folds to use for prediction. Default is None which\n",
            "                        means that folds will be detected automatically in the\n",
            "                        model output folder\n",
            "  -z, --save_npz        use this if you want to ensemble these predictions\n",
            "                        with those of other models. Softmax probabilities will\n",
            "                        be saved as compresed numpy arrays in output_folder\n",
            "                        and can be merged between output_folders with\n",
            "                        merge_predictions.py\n",
            "  -l LOWRES_SEGMENTATIONS, --lowres_segmentations LOWRES_SEGMENTATIONS\n",
            "                        if model is the highres stage of the cascade then you\n",
            "                        need to use -l to specify where the segmentations of\n",
            "                        the corresponding lowres unet are. Here they are\n",
            "                        required to do a prediction\n",
            "  --part_id PART_ID     Used to parallelize the prediction of the folder over\n",
            "                        several GPUs. If you want to use n GPUs to predict\n",
            "                        this folder you need to run this command n times with\n",
            "                        --part_id=0, ... n-1 and --num_parts=n (each with a\n",
            "                        different GPU (for example via CUDA_VISIBLE_DEVICES=X)\n",
            "  --num_parts NUM_PARTS\n",
            "                        Used to parallelize the prediction of the folder over\n",
            "                        several GPUs. If you want to use n GPUs to predict\n",
            "                        this folder you need to run this command n times with\n",
            "                        --part_id=0, ... n-1 and --num_parts=n (each with a\n",
            "                        different GPU (via CUDA_VISIBLE_DEVICES=X)\n",
            "  --num_threads_preprocessing NUM_THREADS_PREPROCESSING\n",
            "                        Determines many background processes will be used for\n",
            "                        data preprocessing. Reduce this if you run into out of\n",
            "                        memory (RAM) problems. Default: 6\n",
            "  --num_threads_nifti_save NUM_THREADS_NIFTI_SAVE\n",
            "                        Determines many background processes will be used for\n",
            "                        segmentation export. Reduce this if you run into out\n",
            "                        of memory (RAM) problems. Default: 2\n",
            "  --tta TTA             Set to 0 to disable test time data augmentation\n",
            "                        (speedup of factor 4(2D)/8(3D)), lower quality\n",
            "                        segmentations\n",
            "  --overwrite_existing OVERWRITE_EXISTING\n",
            "                        Set this to 0 if you need to resume a previous\n",
            "                        prediction. Default: 1 (=existing segmentations in\n",
            "                        output_folder will be overwritten)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NujgNKHarzLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_FOLDER='/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhIc04N0r_j_",
        "colab_type": "code",
        "outputId": "c4244e27-1fb2-4870-c500-e30e6d4cbd3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "!ls $INPUT_FOLDER"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "patientID10_0000.nii.gz  patientID21_0000.nii.gz  patientID28_0000.nii.gz\n",
            "patientID1_0000.nii.gz\t patientID22_0000.nii.gz  patientID29_0000.nii.gz\n",
            "patientID14_0000.nii.gz  patientID23_0000.nii.gz  patientID30_0000.nii.gz\n",
            "patientID16_0000.nii.gz  patientID24_0000.nii.gz  patientID5_0000.nii.gz\n",
            "patientID18_0000.nii.gz  patientID25_0000.nii.gz  patientID6_0000.nii.gz\n",
            "patientID19_0000.nii.gz  patientID26_0000.nii.gz  patientID8_0000.nii.gz\n",
            "patientID2_0000.nii.gz\t patientID27_0000.nii.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JTssmQRsalH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_FOLDER='/content/data/output/'\n",
        "!mkdir $OUTPUT_FOLDER"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voqMzun4sMQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls $OUTPUT_FOLDER"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf9hZKqCrNph",
        "colab_type": "code",
        "outputId": "1f91e1bf-18b8-40dc-dbbe-aa16cfcc768e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!OMP_NUM_THREADS=1 python inference/predict_simple.py -i $INPUT_FOLDER -o $OUTPUT_FOLDER -t Task00_MY_DATASET -f all -tr nnUNetTrainer -m 3d_fullres"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, Fabian, et al. \"nnU-Net: Breaking the Spell on Successful Medical Image Segmentation.\" arXiv preprint arXiv:1904.08128 (2019).\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "using model stored in  /content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans\n",
            "emptying cuda cache\n",
            "loading parameters for folds, ['all']\n",
            "using the following model files:  ['/content/RESULTS_FOLDER/3d_fullres/Task00_MY_DATASET/nnUNetTrainer__nnUNetPlans/all/model_best.model']\n",
            "starting preprocessing generator\n",
            "starting prediction...\n",
            "preprocessing /content/data/output/patientID1.nii.gz\n",
            "preprocessing /content/data/output/patientID10.nii.gz\n",
            "before crop: (1, 36, 89, 89) after crop: (1, 36, 88, 88) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "preprocessing /content/data/output/patientID14.nii.gz\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 36, 88, 88)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 36, 88, 88)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID1.nii.gz OrderedDict([('original_size_of_raw_data', array([36, 89, 89])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID1_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-167.78590393066406, -176.04379272460938, -84.0)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 36], [0, 88], [0, 88]]), ('classes', array([-1,  0])), ('size_after_cropping', (36, 88, 88)), ('size_after_resampling', (36, 88, 88)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 36, 88, 88)\n",
            "preprocessing /content/data/output/patientID2.nii.gz\n",
            "preprocessing /content/data/output/patientID16.nii.gz\n",
            "before crop: (1, 38, 73, 73) after crop: (1, 38, 73, 73) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 38, 73, 73)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 38, 73, 73)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID14.nii.gz OrderedDict([('original_size_of_raw_data', array([38, 73, 73])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID14_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-139.5, -32.5, 1669.0)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 38], [0, 73], [0, 73]]), ('classes', array([-1,  0])), ('size_after_cropping', (38, 73, 73)), ('size_after_resampling', (38, 73, 73)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 38, 73, 73)\n",
            "before crop: (1, 44, 89, 89) after crop: (1, 44, 89, 89) spacing: [4. 4. 4.] \n",
            "\n",
            "preprocessing /content/data/output/patientID18.nii.gz\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 44, 89, 89)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 44, 89, 89)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID10.nii.gz OrderedDict([('original_size_of_raw_data', array([44, 89, 89])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID10_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-159.5, -98.5, 1682.5)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 44], [0, 89], [0, 89]]), ('classes', array([-1,  0])), ('size_after_cropping', (44, 89, 89)), ('size_after_resampling', (44, 89, 89)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "before crop: (1, 41, 100, 100) after crop: (1, 41, 100, 100) spacing: [4. 4. 4.] \n",
            "(1, 44, 89, 89)\n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 41, 100, 100)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 41, 100, 100)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID2.nii.gz OrderedDict([('original_size_of_raw_data', array([ 41, 100, 100])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID2_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-191.0, -111.0, 1655.0)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 41], [0, 100], [0, 100]]), ('classes', array([-1,  0])), ('size_after_cropping', (41, 100, 100)), ('size_after_resampling', (41, 100, 100)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 41, 100, 100)\n",
            "preprocessing /content/data/output/patientID19.nii.gz\n",
            "predicting /content/data/output/patientID1.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "preprocessing /content/data/output/patientID22.nii.gz\n",
            "before crop: (1, 45, 88, 88) after crop: (1, 45, 88, 88) spacing: [4. 4. 4.] \n",
            "\n",
            "before crop: (1, 36, 86, 86) after crop: (1, 36, 86, 86) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 36, 86, 86)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 36, 86, 86)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID19.nii.gz OrderedDict([('original_size_of_raw_data', array([36, 86, 86])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID19_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-157.5, -53.5, 1597.5)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 36], [0, 86], [0, 86]]), ('classes', array([-1,  0])), ('size_after_cropping', (36, 86, 86)), ('size_after_resampling', (36, 86, 86)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 36, 86, 86)\n",
            "no resampling necessary\n",
            "before crop: (1, 46, 102, 102) after crop: (1, 46, 102, 102) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 45, 88, 88)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 45, 88, 88)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "before crop: (1, 41, 93, 93) after crop: (1, 41, 93, 93) spacing: [4. 4. 4.] \n",
            "\n",
            "/content/data/output/patientID18.nii.gz OrderedDict([('original_size_of_raw_data', array([45, 88, 88])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID18_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-173.0, -102.0, 1664.0)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 45], [0, 88], [0, 88]]), ('classes', array([-1,  0])), ('size_after_cropping', (45, 88, 88)), ('size_after_resampling', (45, 88, 88)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 45, 88, 88)\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 46, 102, 102)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 46, 102, 102)} \n",
            "\n",
            "normalization...\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 41, 93, 93)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 41, 93, 93)} \n",
            "\n",
            "normalization done\n",
            "normalization...\n",
            "/content/data/output/patientID16.nii.gz OrderedDict([('original_size_of_raw_data', array([ 46, 102, 102])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID16_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-190.5, -103.5, 1701.0)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 46], [0, 102], [0, 102]]), ('classes', array([-1,  0])), ('size_after_cropping', (46, 102, 102)), ('size_after_resampling', (46, 102, 102)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 46, 102, 102)\n",
            "normalization done\n",
            "/content/data/output/patientID22.nii.gz OrderedDict([('original_size_of_raw_data', array([41, 93, 93])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID22_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-183.0, -89.0, 1573.5)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 41], [0, 93], [0, 93]]), ('classes', array([-1,  0])), ('size_after_cropping', (41, 93, 93)), ('size_after_resampling', (41, 93, 93)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 41, 93, 93)\n",
            "predicting /content/data/output/patientID14.nii.gz\n",
            "preprocessing /content/data/output/patientID21.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "before crop: (1, 65, 100, 100) after crop: (1, 65, 100, 100) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 65, 100, 100)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 65, 100, 100)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID21.nii.gz OrderedDict([('original_size_of_raw_data', array([ 65, 100, 100])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID21_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-211.32545471191406, -74.09186553955078, 9.5)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 65], [0, 100], [0, 100]]), ('classes', array([-1,  0])), ('size_after_cropping', (65, 100, 100)), ('size_after_resampling', (65, 100, 100)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 65, 100, 100)\n",
            "preprocessing /content/data/output/patientID26.nii.gz\n",
            "predicting /content/data/output/patientID10.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "before crop: (1, 46, 98, 98) after crop: (1, 46, 98, 98) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 46, 98, 98)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 46, 98, 98)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID26.nii.gz OrderedDict([('original_size_of_raw_data', array([46, 98, 98])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID26_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-190.0, -106.0, 1598.5)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 46], [0, 98], [0, 98]]), ('classes', array([-1,  0])), ('size_after_cropping', (46, 98, 98)), ('size_after_resampling', (46, 98, 98)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 46, 98, 98)\n",
            "predicting /content/data/output/patientID2.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "preprocessing /content/data/output/patientID25.nii.gz\n",
            "before crop: (1, 59, 77, 77) after crop: (1, 59, 77, 77) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 59, 77, 77)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 59, 77, 77)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID25.nii.gz OrderedDict([('original_size_of_raw_data', array([59, 77, 77])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID25_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-149.0, 4.0, -107.5)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 59], [0, 77], [0, 77]]), ('classes', array([-1,  0])), ('size_after_cropping', (59, 77, 77)), ('size_after_resampling', (59, 77, 77)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 59, 77, 77)\n",
            "predicting /content/data/output/patientID19.nii.gz\n",
            "preprocessing /content/data/output/patientID24.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "before crop: (1, 62, 82, 82) after crop: (1, 62, 82, 82) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 62, 82, 82)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 62, 82, 82)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID24.nii.gz OrderedDict([('original_size_of_raw_data', array([62, 82, 82])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID24_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-176.0, -1.0, -235.20001220703125)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 62], [0, 82], [0, 82]]), ('classes', array([-1,  0])), ('size_after_cropping', (62, 82, 82)), ('size_after_resampling', (62, 82, 82)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 62, 82, 82)\n",
            "predicting /content/data/output/patientID18.nii.gz\n",
            "preprocessing /content/data/output/patientID23.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "before crop: (1, 60, 85, 85) after crop: (1, 60, 85, 85) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 60, 85, 85)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 60, 85, 85)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID23.nii.gz OrderedDict([('original_size_of_raw_data', array([60, 85, 85])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID23_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-152.5, 1.5, -44.29998779296875)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 60], [0, 85], [0, 85]]), ('classes', array([-1,  0])), ('size_after_cropping', (60, 85, 85)), ('size_after_resampling', (60, 85, 85)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 60, 85, 85)\n",
            "predicting /content/data/output/patientID16.nii.gz\n",
            "preprocessing /content/data/output/patientID28.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "before crop: (1, 41, 89, 89) after crop: (1, 41, 89, 89) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 41, 89, 89)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 41, 89, 89)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID28.nii.gz OrderedDict([('original_size_of_raw_data', array([41, 89, 89])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID28_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-167.5, -92.5, 1570.0)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 41], [0, 89], [0, 89]]), ('classes', array([-1,  0])), ('size_after_cropping', (41, 89, 89)), ('size_after_resampling', (41, 89, 89)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 41, 89, 89)\n",
            "preprocessing /content/data/output/patientID27.nii.gz\n",
            "predicting /content/data/output/patientID22.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "before crop: (1, 61, 88, 88) after crop: (1, 61, 88, 88) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 61, 88, 88)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 61, 88, 88)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID27.nii.gz OrderedDict([('original_size_of_raw_data', array([61, 88, 88])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID27_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-183.0, -36.0, -62.39996337890625)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 61], [0, 88], [0, 88]]), ('classes', array([-1,  0])), ('size_after_cropping', (61, 88, 88)), ('size_after_resampling', (61, 88, 88)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 61, 88, 88)\n",
            "predicting /content/data/output/patientID21.nii.gz\n",
            "preprocessing /content/data/output/patientID6.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "before crop: (1, 33, 84, 84) after crop: (1, 33, 84, 84) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 33, 84, 84)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 33, 84, 84)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID6.nii.gz OrderedDict([('original_size_of_raw_data', array([33, 84, 84])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID6_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-169.28729248046875, -166.2845001220703, -40.5)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 33], [0, 84], [0, 84]]), ('classes', array([-1,  0])), ('size_after_cropping', (33, 84, 84)), ('size_after_resampling', (33, 84, 84)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 33, 84, 84)\n",
            "preprocessing /content/data/output/patientID5.nii.gz\n",
            "predicting /content/data/output/patientID26.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "before crop: (1, 38, 97, 97) after crop: (1, 38, 97, 97) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 38, 97, 97)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 38, 97, 97)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID5.nii.gz OrderedDict([('original_size_of_raw_data', array([38, 97, 97])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID5_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-171.5, -126.5, 1683.0)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 38], [0, 97], [0, 97]]), ('classes', array([-1,  0])), ('size_after_cropping', (38, 97, 97)), ('size_after_resampling', (38, 97, 97)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 38, 97, 97)\n",
            "preprocessing /content/data/output/patientID30.nii.gz\n",
            "predicting /content/data/output/patientID25.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "before crop: (1, 67, 94, 94) after crop: (1, 67, 94, 94) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 67, 94, 94)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 67, 94, 94)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID30.nii.gz OrderedDict([('original_size_of_raw_data', array([67, 94, 94])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID30_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-213.5, -64.5, -18.70001220703125)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 67], [0, 94], [0, 94]]), ('classes', array([-1,  0])), ('size_after_cropping', (67, 94, 94)), ('size_after_resampling', (67, 94, 94)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 67, 94, 94)\n",
            "preprocessing /content/data/output/patientID29.nii.gz\n",
            "predicting /content/data/output/patientID24.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "before crop: (1, 54, 88, 88) after crop: (1, 54, 88, 88) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 54, 88, 88)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 54, 88, 88)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID29.nii.gz OrderedDict([('original_size_of_raw_data', array([54, 88, 88])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID29_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-196.0, -28.0, -68.50003051757812)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 54], [0, 88], [0, 88]]), ('classes', array([-1,  0])), ('size_after_cropping', (54, 88, 88)), ('size_after_resampling', (54, 88, 88)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 54, 88, 88)\n",
            "predicting /content/data/output/patientID23.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "preprocessing /content/data/output/patientID8.nii.gz\n",
            "predicting /content/data/output/patientID28.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "before crop: (1, 50, 94, 94) after crop: (1, 49, 94, 94) spacing: [4. 4. 4.] \n",
            "\n",
            "no resampling necessary\n",
            "no resampling necessary\n",
            "before: {'spacing': array([4., 4., 4.]), 'spacing_transposed': array([4., 4., 4.]), 'data.shape (data is transposed)': (1, 49, 94, 94)} \n",
            "after:  {'spacing': array([4., 4., 4.]), 'data.shape (data is resampled)': (1, 49, 94, 94)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "/content/data/output/patientID8.nii.gz OrderedDict([('original_size_of_raw_data', array([50, 94, 94])), ('original_spacing', array([4., 4., 4.])), ('list_of_data_files', ['/content/nnUNet_base/nnUNet_raw_splitted/Task00_MY_DATASET/imagesTr/patientID8_0000.nii.gz']), ('seg_file', None), ('itk_origin', (-176.0, -95.0, 1679.5)), ('itk_spacing', (4.0, 4.0, 4.0)), ('itk_direction', (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0)), ('crop_bbox', [[0, 49], [0, 94], [0, 94]]), ('classes', array([-1,  0])), ('size_after_cropping', (49, 94, 94)), ('size_after_resampling', (49, 94, 94)), ('spacing_after_resampling', array([4., 4., 4.]))])\n",
            "(1, 49, 94, 94)\n",
            "predicting /content/data/output/patientID27.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "predicting /content/data/output/patientID6.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "predicting /content/data/output/patientID5.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "predicting /content/data/output/patientID30.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "This worker has ended successfully, no errors to report\n",
            "predicting /content/data/output/patientID29.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "This worker has ended successfully, no errors to report\n",
            "predicting /content/data/output/patientID8.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "This worker has ended successfully, no errors to report\n",
            "This worker has ended successfully, no errors to report\n",
            "This worker has ended successfully, no errors to report\n",
            "This worker has ended successfully, no errors to report\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr000sfmtFiP",
        "colab_type": "code",
        "outputId": "abdce09a-bbb2-4515-840b-f75f423f5a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import colab_transfer\n",
        "\n",
        "colab_transfer.copy_folder_structure(\n",
        "    '/content/data/',\n",
        "    '/content/drive/My Drive/downsampled_data/',    \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/data/imagesTr', '/content/data/labelsTr', '/content/data/output']\n",
            "[]\n",
            "/content/data/imagesTr\n",
            "imagesTr/\n",
            "patientID2_0000.nii.gz\n",
            "Copying patientID2_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID24_0000.nii.gz\n",
            "Copying patientID24_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID5_0000.nii.gz\n",
            "Copying patientID5_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID23_0000.nii.gz\n",
            "Copying patientID23_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID6_0000.nii.gz\n",
            "Copying patientID6_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID18_0000.nii.gz\n",
            "Copying patientID18_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID8_0000.nii.gz\n",
            "Copying patientID8_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID27_0000.nii.gz\n",
            "Copying patientID27_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID10_0000.nii.gz\n",
            "Copying patientID10_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID25_0000.nii.gz\n",
            "Copying patientID25_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID1_0000.nii.gz\n",
            "Copying patientID1_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID22_0000.nii.gz\n",
            "Copying patientID22_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID26_0000.nii.gz\n",
            "Copying patientID26_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID16_0000.nii.gz\n",
            "Copying patientID16_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID14_0000.nii.gz\n",
            "Copying patientID14_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID30_0000.nii.gz\n",
            "Copying patientID30_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID19_0000.nii.gz\n",
            "Copying patientID19_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID29_0000.nii.gz\n",
            "Copying patientID29_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID21_0000.nii.gz\n",
            "Copying patientID21_0000.nii.gz\n",
            "imagesTr/\n",
            "patientID28_0000.nii.gz\n",
            "Copying patientID28_0000.nii.gz\n",
            "/content/data/labelsTr\n",
            "labelsTr/\n",
            "patientID22.nii.gz\n",
            "Copying patientID22.nii.gz\n",
            "labelsTr/\n",
            "patientID25.nii.gz\n",
            "Copying patientID25.nii.gz\n",
            "labelsTr/\n",
            "patientID1.nii.gz\n",
            "Copying patientID1.nii.gz\n",
            "labelsTr/\n",
            "patientID8.nii.gz\n",
            "Copying patientID8.nii.gz\n",
            "labelsTr/\n",
            "patientID24.nii.gz\n",
            "Copying patientID24.nii.gz\n",
            "labelsTr/\n",
            "patientID19.nii.gz\n",
            "Copying patientID19.nii.gz\n",
            "labelsTr/\n",
            "patientID26.nii.gz\n",
            "Copying patientID26.nii.gz\n",
            "labelsTr/\n",
            "patientID14.nii.gz\n",
            "Copying patientID14.nii.gz\n",
            "labelsTr/\n",
            "patientID18.nii.gz\n",
            "Copying patientID18.nii.gz\n",
            "labelsTr/\n",
            "patientID2.nii.gz\n",
            "Copying patientID2.nii.gz\n",
            "labelsTr/\n",
            "patientID21.nii.gz\n",
            "Copying patientID21.nii.gz\n",
            "labelsTr/\n",
            "patientID23.nii.gz\n",
            "Copying patientID23.nii.gz\n",
            "labelsTr/\n",
            "patientID10.nii.gz\n",
            "Copying patientID10.nii.gz\n",
            "labelsTr/\n",
            "patientID30.nii.gz\n",
            "Copying patientID30.nii.gz\n",
            "labelsTr/\n",
            "patientID28.nii.gz\n",
            "Copying patientID28.nii.gz\n",
            "labelsTr/\n",
            "patientID5.nii.gz\n",
            "Copying patientID5.nii.gz\n",
            "labelsTr/\n",
            "patientID6.nii.gz\n",
            "Copying patientID6.nii.gz\n",
            "labelsTr/\n",
            "patientID16.nii.gz\n",
            "Copying patientID16.nii.gz\n",
            "labelsTr/\n",
            "patientID27.nii.gz\n",
            "Copying patientID27.nii.gz\n",
            "labelsTr/\n",
            "patientID29.nii.gz\n",
            "Copying patientID29.nii.gz\n",
            "/content/data/output\n",
            "output/\n",
            "patientID22.nii.gz\n",
            "Copying patientID22.nii.gz\n",
            "output/\n",
            "patientID25.nii.gz\n",
            "Copying patientID25.nii.gz\n",
            "output/\n",
            "patientID1.nii.gz\n",
            "Copying patientID1.nii.gz\n",
            "output/\n",
            "patientID8.nii.gz\n",
            "Copying patientID8.nii.gz\n",
            "output/\n",
            "patientID24.nii.gz\n",
            "Copying patientID24.nii.gz\n",
            "output/\n",
            "patientID19.nii.gz\n",
            "Copying patientID19.nii.gz\n",
            "output/\n",
            "patientID26.nii.gz\n",
            "Copying patientID26.nii.gz\n",
            "output/\n",
            "patientID14.nii.gz\n",
            "Copying patientID14.nii.gz\n",
            "output/\n",
            "patientID18.nii.gz\n",
            "Copying patientID18.nii.gz\n",
            "output/\n",
            "patientID2.nii.gz\n",
            "Copying patientID2.nii.gz\n",
            "output/\n",
            "patientID21.nii.gz\n",
            "Copying patientID21.nii.gz\n",
            "output/\n",
            "patientID23.nii.gz\n",
            "Copying patientID23.nii.gz\n",
            "output/\n",
            "patientID10.nii.gz\n",
            "Copying patientID10.nii.gz\n",
            "output/\n",
            "patientID30.nii.gz\n",
            "Copying patientID30.nii.gz\n",
            "output/\n",
            "patientID28.nii.gz\n",
            "Copying patientID28.nii.gz\n",
            "output/\n",
            "patientID5.nii.gz\n",
            "Copying patientID5.nii.gz\n",
            "output/\n",
            "patientID6.nii.gz\n",
            "Copying patientID6.nii.gz\n",
            "output/\n",
            "plans.pkl\n",
            "Copying plans.pkl\n",
            "output/\n",
            "patientID16.nii.gz\n",
            "Copying patientID16.nii.gz\n",
            "output/\n",
            "patientID27.nii.gz\n",
            "Copying patientID27.nii.gz\n",
            "output/\n",
            "patientID29.nii.gz\n",
            "Copying patientID29.nii.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GItq5R6v5S3W",
        "colab_type": "text"
      },
      "source": [
        "## Visualize segmentation results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNWntxcMtOpx",
        "colab_type": "text"
      },
      "source": [
        "[DONE] Download data and display segmentation overlayed on CT images with ITK-SNAP."
      ]
    }
  ]
}